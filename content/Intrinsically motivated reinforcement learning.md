---
aliases:
  - intrinsically motivated reinforcement learning
  - IMRL
tags:
  - permanent-note
  - topic-computer-science-information
publish: "true"
date: 2023-12-29 18:17
lastmod: 2023-12-29T19:03:17-08:00
---
In the “standard” view of RL, the reward comes from an external “critic” in the environment that evaluates the agent’s behavior. This is different from biological cognition, where an animal determines its reward from both its external and internal state: “The critic is in an animal’s head” ([[intrinsically-motivated-reinforcement-learning|Singh et al., 2004]]). 

[[intrinsically-motivated-reinforcement-learning|Singh et al. (2004)]] first implemented a simple notion of intrinsic reward: the amount of surprise generated by salient events. They found that “an agent having a collection of skills learned through intrinsic reward can learn a wide variety of extrinsically rewarded tasks more easily than an agent lacking these skills”; that is, similar to animals, skills learned without an explicit reward enabled [[Generalization|generalization]].

---
# References

[[intrinsically-motivated-reinforcement-learning]]

---
# Log

2023-12-29
- This seems to be the opposite problem of [[Embodied cognition|embodiment]] in ML; the former is about putting the computation “inside” the agent, the latter is about extending cognition beyond the individual.