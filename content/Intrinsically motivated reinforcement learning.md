---
aliases:
  - intrinsically motivated reinforcement learning
  - IMRL
tags:
  - permanent-note
  - topic-computer-science-information
publish: "true"
date: 2023-12-29 18:17
lastmod: 2023-12-29T19:21:26-08:00
---
In the “standard” view of RL, the reward comes from an external “critic” in the environment that evaluates the agent’s behavior. This is different from biological cognition, where an animal determines its reward from both its external and internal state: “The critic is in an animal’s head” ([[intrinsically-motivated-reinforcement-learning|Singh et al., 2004]]). 

[[intrinsically-motivated-reinforcement-learning|Singh et al. (2004)]] first implemented a simple notion of intrinsic reward: the amount of surprise generated by salient events. They found that “an agent having a collection of skills learned through intrinsic reward can learn a wide variety of extrinsically rewarded tasks more easily than an agent lacking these skills”; that is, similar to animals, skills learned without an explicit reward enabled [[Generalization|generalization]].

## Approaches to IMRL

- Curiosity (Pathak et al., 2017; Schmidhuber, 2010)
- #queue [Empowerment](https://towardsdatascience.com/empowerment-as-intrinsic-motivation-b84af36d5616) (Klyubin et al., 2005)
- Intrinsic social motivation via MARL ([[social-influence-as-intrinsic-motivation|Jaques et al., 2019]])

---
# References

[[intrinsically-motivated-reinforcement-learning]]

---
# Log

2023-12-29
- This seems to be the opposite problem of [[Embodied cognition|embodiment]] in ML; the former is about putting the computation “inside” the agent, the latter is about extending cognition beyond the individual.