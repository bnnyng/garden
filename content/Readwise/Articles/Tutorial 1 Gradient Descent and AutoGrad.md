# Tutorial 1: Gradient Descent and AutoGrad

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)

# Metadata
- Author: [[neuromatch.io]]
- Full Title: Tutorial 1: Gradient Descent and AutoGrad

- URL: https://deeplearning.neuromatch.io/tutorials/W1D2_LinearDeepLearning/student/W1D2_Tutorial1.html#video-4-auto-differentiation

# Highlights
- For gradient descent, it is only required to have the gradients of cost function with respect to the variables we wish to learn. These variables are often called “learnable / trainable parameters” or simply “parameters” in PyTorch. In neural nets, weights and biases are often the learnable parameters. ([View Highlight](https://read.readwise.io/read/01h9pgbbmcj6x8jaww12wytj2b))
