---
aliases:
  - "Discussion Post: Functionalism and the Computational Theory of Mind (CTM)"
tags:
  - fleeting-note
date: 2024-10-03 02:54
lastmod: 2024-10-03T12:53:53-04:00
---
# Sketch

- Functional kind has only nominal essence
	- Self-reference. I am not troubled by this. It is actually the only reasonable thing
- Mental state has genuine ontological status (mental realism) — what is a mental state if not conscious?
- Symbol grounding problem — how a representation can come to mean anything at all. 
	- Certainly, Searle’s Chinese room has this issue (“holistic conception of mentality”)
	- How can we have a general theory of information processing with semantic information?
- Isomorphism between two cognitive systems sharing a same state
	- Completely determined by inputs – then the system is structurally coupled with the enviornment
	- Okay isomorphic with respect to their inputs. A structure-preserving map. But the program is also a map from state to state. Homotopic?
		- Requires isomorphism between inputs and outputs, and homotopy between the program and map
	- How can states be discrete?

---
# Discussion post

I find that all these computational theories of mind, including machine functionalism, leave something to be desired. (I cannot yet concisely say what it is but maybe I’ll build up to it via small critiques.) First, a classical Turing machine has countably many states (since it is representable by a table, possibly infinite), but it is unclear to me why we should assume that psychological states are also countable, and thus can be validly mapped to computers. I only have the data of my own conscious experience, but given any mental state I experience, I feel like I can find another mental state arbitrarily “causally” close to it, i.e., my mental causal processes are continuous. Okay, suppose both minds and computers have uncountably many intermediary causal states. Then, since the causal processes mapping inputs to outputs are continuous, we no longer have what Kim calls the isomorphism problem for machine functionalism. Two sets are isomorphic if they preserve the same structure (thus, the problem is that two systems sharing one mental state must share the whole program); two maps are homotopic if they can be continuously deformed into one other. I also wish the idea of programs being defined in relation to their inputs was more explored. I took this to mean that each computational process is somewhat coupled to its environment, i.e., encoding information about the external world in its own physical structure (maybe also softening the symbol grounding-type problem Kim mentions). This suggests to me that if two (countable) programs are isomorphic, then there is some isomorphism in their inputs as well. A continuous view of computation puts an interesting spin on this, since homotopies induce isomorphisms; then two mental processes that can be continuous deformed into one another will indicate redundancies in physical reality. Is redunancy the key to meaningful information?

---
# Lecture notes

- [[The Turing test]]
	- Sufficient condition for intelligence, but not necessary
	- Concepts, Wittgenstein, meaning as use
	- You can pass it by a lot of “cheap tricks”—but at one point do those add up to intelligence?
	- LLMs should have a “cognitive map”—conversely, fail on tasks that probe deeper into “intuitive physics”
- Multiple realizability = defining something more generally using its function
	- Functionally defined $\implies$ multiply realized
	- Example: predator and prey