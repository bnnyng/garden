---
tags:
  - fleeting-note
lastmod: 2024-02-07T12:11:01-08:00
---
# Pragmatic statistics

>Whenever an analysis starts touching on some sort of data, the field of statistics must get involved. Statistics can be used for exploration, prediction, and understanding, but what sort of statistical tools are applicable to each of these goals? Moreover, as soon as reality and custom models to approximate it get involved, a lot of statistical theory and many classical methods become useless. We will discuss powerful statistical tools (including regression and its generalizations, bootstrapping, and Bayesian methods) which can be immediately applied by both statisticians and non-statisticians to a wide variety of real-world problems without having to spend weeks developing methods and theory. We will go through the Bayesian modeling and uncertainty quantification done by my team for MCM Problem C in 2023, which was noted in the Problem C Judges' Commentary and for which we received Outstanding Winner.

- About Anthony Ozerov
	- Stats & CS undergrad, current stats PhD at Berkeley
- What is statistics for? – 3 tasks of statisticians
	- Description = coming up with facts based on the data
	- Prediction = know something about a different dataset that we have not seen
	- Inference = trying to understand a property of the world; causal statements, interpretations
	- Moving down means more assumptions → criticism, mistakes, difficulty, but also understanding!
- **Before deciding what methods to use on a problem, decide what the goal is.**
	- Get a handle on data = description
	- Make claims about new/unseen observations = prediction
	- Understand real-world phenomena = inference
- Descriptive statements concern only our data and nothing outside of our data
	- What does the data look like? What should we study?
	- Benefits of description:
		- NO assumptions – “this is what I see”
		- No math; sometimes staring at graph is sufficient to ask more interesting questions about the dataset
		- Show which questions to ask
		- Inform which statistical methods to use
	- Tools for description:
		- Graphs: scatterplots, histogram
		- Numbers like mean, median, SD
		- Anything to **visualize or summarize** the data; also called “exploratory data analysis”
		- PCA = projects data into lower dimensions to get information about structure
- Predictive statements are about what new, unseen observations will look like (outside data, but hopefully similar)
	- Example: MCM C for Wordle results
	- Confidence intervals reflect level of belief in a prediction
	- Difficulties about prediction:
		- Prediction is not the same as understanding (e.g., does black box mean “understanding”?)
			- For certain problems, is just being able to make predictions enough?
	- Assumptions can be invalidating, but not a problem if we use a test set
		- Is the test set representative of what we will use the model to predict for?
	- Tools for prediction:
		- Regression, fancier regression
		- Bayesian models
		- Neural networks
		- Machine learning in general
	- You can make testable predictions without statistics?
- Inference is making a statement about the world outside the data
	- MCMC gives approximation of distribution, tails are less smooth
	- Considerations with inference:
		- If not testable predictions, can’t know if right
		- Assumptions probably won’t hold in real life
		- Different seemingly useful methods will give different answers
		- Need **sensitivity and stability analysis** – vary different parts of model to see how sensitive the inference is to small changes in the model
	- **Any trustworthy inference must have quantified uncertainty (e.g., probability).**
		- What does probability mean?
			- Frequentist = long-run frequency
			- Bayesian = subjective belief; there is still a probability even once the outcome is determined, as long as we don’t know
			- Upshot: think about what the statements actually mean
	- Inference is understanding an aspect of the real world $\theta$
		- Causal effect
		- Coefficient in a linear relationship
		- Classification of different objects
		- A function
		- Any sort of number in the world
	- Representing an inference
		- Point estimate = single estimate for $\theta$ that is hopefully close to the true value; how to talk about uncertainty?
		- Frequentist uncertainty = confidence intervals, sampling distributions
		- Bayesian = credible interval, posterior distribution
	- Frequentist vs Bayesian uncertainty
		- What is $\theta$
			- F: $\theta$ is an UNKNOWN QUANTITY already determined in some way
			- B: $\theta$ is a RANDOM VARIABLE that we already have a prior distribution on
		- What is the distribution
			- F: distribution of the estimates after running the experiments a bunch of time
			- B: belief about $\theta$ now that the data has been seen – posterior distribution of the random variable
		- Intervals
			- F: “confidence interval” which contains $\theta$ with a certain % confidence
			- B: “credible interval”; we think $\theta$ is in this interval with certain % probability
- Widely applicable statistical tools
	- Regression = draw a line through a bunch of points
		- Can use a vector of predictors, and function can be any function
		- Don’t feel constrained by linearity!
		- We assume the noise is normally distributed, implicitly
			- We can replace normal with any other distribution
		- Choosing parameters
			- F: Loss function, pick coefficients which minimize loss; automatically done by e.g., `scikit-learn`
			- B: put priors on parameters, use MCMC to find posterior distributions of coefficients
	- Bootstrap = give confidence intervals on anything you want when there is not an explicit formula
		- A CI comes from the sampling distribution of the estimator – not always feasible to calculate
			- Simulate what different random samples would look like and recompute estimates (resample/draw with replacement)
			- For each sample, compute the estimator to get a sampling distribution
			- Then we can compute the interval
		- For multiple parameters, can have confidence region
	- Bayesian modeling = given prior distribution $p(\theta)$, new data and likelihood $p(X | \theta)$, the new belief should be the posterior distribution $p (\theta|X)$.
		- Prior and posterior distributions come from same family
		- In practice, think about the data-generating practice
			- Example: for wordle, # of tries comes from Multinomial distribution, but need probability parameter from Dirichlet, which has things coming from normal distribution? and then the average “difficulty” comes from a linear combination of predictors
		- Choosing priors
			- No prior is truly objective or uninformative
			- If parameter is bounded, prior should be bounded
			- Priors centered at zero can offer regualrization
		- Getting posterior
			- Use MCMC to get samples from posterior distribution – don’t need a function for posterior density
				- Check that samples converge to posterior distribution
		- What to do with posterior samples
			- Each sample is a vector that contains each parameter of the model
			- Can automatically get samples of any function of the parameters by applying functions to the samples
			- Can apply prediction parameters to get a posterior outcome
- Be clear about goal: description, prediction, or inference!

---

# Practical optimization

>At a high level, optimization seeks to answer the question of “what’s the best we can do” while subject to some set of constraints and hence is indispensable for almost any real-world modeling problem. Unfortunately, tools from rigorous optimization theory are often not very useful, assuming underlying structure about problems that tend not to hold in practice. In this talk, we will first discuss guiding principles from optimization theory but very quickly pivot towards practical considerations in problem formulation (e.g. constructing utility functions, parameter setting, discretization, computational tractability) and practical algorithms/tools to compute their solutions (e.g. variations of hill climbing algorithms, greedy algorithms, dynamic programming, scipy.optimize). We will also touch upon specific real-world problems and how these optimization methods were applied to them, including for my team’s solutions to past CMMC and MCM problems relating to New York City’s trash crisis and optimizing cyclist race strategies.

- **Optimization** = the field that answers, subject to some constraints, what is the best we can do?
	- Canonical example: the diet problem
		- Introduction to linear programming
		- Given $n$ different foods each with an associated nutrient, what’s the optimal diet for minimizing cost?
		- Have a linear function with the literal cost of each food, subject to certain nutrient constraints
		- Solve for the amount of each food needed ($x$ 1-3) to have the minimal cost
- **Linear programming** = minimize a linear combination of variables, with constraints that are inequalities
	- A solution set is some volume in $n$ dimensional space
	- **Polytope** = a higher-dimensional generalization of polygon
	- Linear program and polygon assumptions lead to theorem that minimum is at a vertex
- Other examples of structured optimization
	- Continuous – convex optimization (convex utility function, not just linear)
	- Discrete – submodular optimization
	- Optimization problems on graphs/networks
- Typical theory assumptions
	- Given two possible solutions, the better solution is obvious (e.g., clearly one costs less)
	- A lot of underlying structure – linearity, convexity, submodularity
	- Usually no uncertainty
- In practice, can’t just “throw the book” at problems!
	- Simplify the problem (modeling)
	- Apply techniques that don’t require rigorous assumptions
	- Settle for “approximately optimal”
- 