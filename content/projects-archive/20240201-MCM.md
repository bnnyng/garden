---
aliases:
  - MCM 2024
tags:
  - project
publish: 
date: 2024-02-01 13:11
lastmod: 2024-02-06T12:42:05-08:00
hours-worked: "54"
---
# Overview

[2024 MCM website](https://www.comapmath.com/MCMICM/index.html)
[LaTeX document](https://www.overleaf.com/project/65b99e2ba9411092ba40c93d)

[[20240104-CUMMW]]
[[20240124-CUMMW-modeling-right-when-the-time-is-tight]]
[[20240131-last-minute-mcm-tips-and-tricks]]

---
# Submission checklist

Formatting
- [ ] No identifying information
- [ ] Acknowledge use of AI
- [ ] Society for Applied Math citations
	- [ ] Alphabetical order, square brackets, hyperlinks, hyper references

Characteristics of outstanding papers
- [ ] **All problem requirements are addressed**
- [ ] Uncertainties are effectively accounted for
- [ ] Results are plausible and well tested/communicated
- [ ] Data visualizations support interpretations and conclusions
- [ ] Model strengths and weaknesses are clearly identified and analyzed
- [ ] Summary sheet elements
	- [ ] Short introduction to the problem – convey problem requirements, give insights into specific results
	- [ ] Summary of primary results
	- [ ] Overview of methods and techniques

>**Clarity and Structure:** Assess the paper's structural clarity by evaluating the logical presentation of main ideas. Provide feedback on the overall flow, identifying areas where additional explanations or transitions could enhance comprehension.

- [ ] Summary and introduction are written for a **non-technical audience**
- [ ] All notation is consistent and defined early
- [ ] All specialized terms are defined
- [ ] No longer than 1-2 paragraphs to explain a model we did not develop. Instead, include citation to lengthy explanations
- [ ] Have sufficient detail for judges to *always* understand rationale

>**Model Assumptions and Justifications:** Evaluate the clarity and adequacy of the model assumptions. Verify if assumptions are explicitly stated and aligned with the problem. Assess the provided justifications and suggest improvements if necessary.

>**Data Analysis and Interpretation:** Examine the team's approach to data analysis. Ensure the appropriate handling of data and alignment of interpretations with the problem statement.

- [ ] Interpret outcomes of analysis in context (e.g., for PCA, what do the resulting components mean for the problem?)

>**Model Validity and Robustness:** Comment on the validity and robustness of the model. Evaluate the team's analysis of result sensitivity to changes in key parameters and conditions.

- [ ] Do not use words like “weakness” when conveying how the model can be improved
- [ ] For a predictive model, provide a **prediction interval** with an associated **confidence level** or measure of uncertainty
- [ ] Validate predictive models on unseen data where true labels are known (e.g., train/test for deterministic models, a single known data point for Bayesian/probabilistic models; a benchmark for “perfect knowledge” in 2022)
- [ ] Sensitivity analysis on any parameter to assess robustness of model to assumptions

>**Presentation and Visualization:** Assess the quality of visual aids and figures. Confirm that graphs and tables are clear, labeled, captioned, and contribute effectively to reader understanding. Offer suggestions for enhancements or additional visualizations.

- [ ] All graphs and figures stand on their own
- [ ] Data visualizations clearly convey what they represent

>**Conclusion and Implications:** Consider the strength of the paper's conclusion. Evaluate its ability to appropriately summarize findings and implications. Provide feedback on any unaddressed limitations and suggest areas for further exploration or refinement.

- [ ] Conclusion is focused on the problem itself and insights
- [ ] Identify what could have been done differently if given more time

---

# Day 5: Help!

## Summary

- [ ] Summary sheet elements
	- [ ] Short introduction to the problem – convey problem requirements, give insights into specific results
	- [ ] Summary of primary results
	- [ ] Overview of methods and techniques

Momentum—the psychological and physical state of a player after a period of high performance—is thought to be highly influential in the flow and outcome of tennis matches. Dramatic momentum “swings” can occur throughout a match… However, few agree on a precise definition of what momentum is, or even if momentum exists at all. 

In this paper, we develop a Markov chain model that uses Bayesian estimation to predict the next score in the game, and pin down precise definitions of *performance* and *momentum* in order to 

The probabilities of transitioning from one score to another in our Markov model are obtained using Bayesian logistic regression. The Bayesian approach seamlessly incorporates prior beliefs 
- Very natural to think of in the context of sports
- Frames our results reasonably


- Aligns naturally with player experience
- Incorporating prior beliefs, such
## Running notes

- Sensitivity analysis of different event types
	- Beginning game = both values are below 30
	- One value is strictly 0 – one person really sucks
	- Both values are above 40
- Copy editing tbd
	- Make sure use of parameters is consistent
	- Make sure use of 

## Data

#### Sensitivity analysis
At least on player is 0
```
At least one player is 0
Brier Score on test set: 0.34831046882362215

Everything 30-30 and to the right, end: 
Brier Score on test set: 0.3513452061050338

Within 1 point of each other but none zero, beginning:
Brier Score on test set: 0.3544888840493735



```
# Day 4: Conducting analyses

## Running notes

9:03 p.m.
- Other graphics:
	- Confusion matrix for benchmark

## Data

#### Fitted model
Priors
```

    # Define priors for coefficients
    intercept = pm.Normal('Intercept', mu=0.619, sigma=0.12)
    prior_fatigue = pm.Normal('B_fatigue', mu=-1.115, sigma = 0.25)
    prior_elo = pm.Normal('B_elo', mu=4.464, sigma = 1)
    prior_speed = pm.Normal('B_speed', mu=1.847, sigma=0.37)
```

Summary statistics
```
            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \
Intercept  0.787  0.027   0.741    0.840      0.000    0.000    5385.0   
B_fatigue -0.599  0.216  -1.001   -0.194      0.003    0.002    6045.0   
B_elo      0.329  0.046   0.241    0.416      0.001    0.000    6325.0   
B_speed    1.133  0.086   0.972    1.293      0.001    0.001    5484.0   
```

Evaluation using the test set
```
Accuracy: 0.6899862825788752
Precision: 0.6971830985915493
Recall: 0.9782608695652174
F1-score: 0.8141447368421051

Brier Score on test set: 0.20875971685789396
Brier Score of benchmark (0.6876): 0.21236687385459532
```
#### Sensitivity analysis 
###### *t* distribution

Intercept - outliers have little effect

```
mean sd
0.792 0.026

Accuracy: 0.6899862825788752
Precision: 0.6971830985915493
Recall: 0.9782608695652174
F1-score: 0.8141447368421051
Brier Score on test set: 0.20871212086925472
```

Fatigue - outliers have significant effect on mean and std, not on outcome scores
```
mean sd
0.413  0.420 

Accuracy: 0.6899862825788752
Precision: 0.6977401129943502
Recall: 0.9762845849802372
F1-score: 0.8138385502471169
Brier Score on test set: 0.2084168778962766
```

Elo - outliers have zero effect
```
mean std
0.320  0.047 

Accuracy: 0.6899862825788752
Precision: 0.6966292134831461
Recall: 0.9802371541501976
F1-score: 0.8144499178981939
Brier Score on test set: 0.20869419438816308
```

Speed - 
```
mean std
1.110 0.094

Accuracy: 0.691358024691358
Precision: 0.697054698457223
Recall: 0.9822134387351779
F1-score: 0.815422477440525
Brier Score on test set: 0.20877048599748838
```

#### Doubled coefficients

Intercept
```
mean std
0.821 0.027
```

Fatigue - varies a lot?
```
mean std
-1.389 0.213

Accuracy: 0.6886145404663924
Precision: 0.6962025316455697
Recall: 0.9782608695652174
F1-score: 0.8134757600657355
Brier Score on test set: 0.20919241093158542
```

Elo - does not vary
```
mean std
0.338 0.046

Accuracy: 0.6899862825788752
Precision: 0.6971830985915493
Recall: 0.9782608695652174
F1-score: 0.8141447368421051
Brier Score on test set: 0.2088089955928328
```

Speed
```
mean std
1.243 0.092
```

## Swing model

Brier scores
```
Brier Score on test set: 0.07270233196159122
Brier Score on prior intercept: 0.06742198396044073
Brier Score on posterior intercept: 0.07279518590721679
```

Before any perturbation
```
            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \
Intercept -1.769  0.080  -1.916   -1.617      0.001    0.001    3733.0   
B_1       -0.518  0.685  -1.773    0.748      0.008    0.010    6849.0   
B_2       -0.516  0.167  -0.824   -0.199      0.002    0.002    6450.0   
B_3       -0.590  0.217  -0.984   -0.170      0.003    0.002    5702.0   
B_4       -0.615  0.810  -2.205    0.801      0.010    0.011    6954.0   
B_5        0.470  0.279  -0.047    0.997      0.004    0.003    5589.0   
B_6       -0.048  0.026  -0.101   -0.002      0.000    0.000    7696.0   
B_7       -0.468  0.248  -0.923    0.004      0.003    0.003    6414.0   
B_8       -0.362  0.062  -0.480   -0.246      0.001    0.001    4100.0   
B_9       -0.439  0.078  -0.580   -0.290      0.001    0.001    4286.0   
B_10      -1.272  0.367  -1.978   -0.612      0.004    0.003    8394.0   
B_11       0.348  0.093   0.167    0.512      0.001    0.001    5968.0   
```

With a larger std
```
             mean      sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \
Intercept  -1.766   0.078  -1.907   -1.617      0.001    0.001    2962.0   
B_1        -1.303   1.298  -3.734    0.987      0.029    0.024    2499.0   
B_2        -0.532   0.166  -0.850   -0.230      0.003    0.002    3670.0   
B_3        -0.594   0.221  -1.017   -0.205      0.004    0.003    3086.0   
B_4       -15.972  11.978 -37.691    1.271      0.344    0.259    1429.0   
B_5         0.496   0.272  -0.029    1.010      0.005    0.004    3073.0   
B_6        -0.047   0.028  -0.099    0.005      0.000    0.000    3520.0   
B_7        -0.460   0.264  -0.983    0.004      0.004    0.003    3654.0   
B_8        -0.362   0.061  -0.473   -0.244      0.001    0.001    3122.0   
B_9        -0.437   0.075  -0.586   -0.306      0.001    0.001    2656.0   
B_10       -1.418   0.433  -2.206   -0.591      0.007    0.006    3794.0   
B_11        0.344   0.091   0.171    0.506      0.002    0.001    2847.0 
```

After perturbation
```
             mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \
Intercept  -2.554  0.239  -2.793   -2.315      0.169    0.143       2.0   
B_1        10.330  0.171  10.159   10.501      0.121    0.102       2.0   
B_2         9.918  0.357   9.561   10.275      0.252    0.213       2.0   
B_3        10.938  0.004  10.934   10.942      0.003    0.002       2.0   
B_4         9.684  0.013   9.671    9.697      0.009    0.008       2.0   
B_5        10.169  0.251   9.918   10.421      0.177    0.150       2.0   
B_6         9.832  0.128   9.703    9.960      0.091    0.077       2.0   
B_7         9.526  0.486   9.040   10.011      0.343    0.290       2.0   
B_8        10.055  0.805   9.250   10.859      0.568    0.481       2.0   
B_9         9.761  0.632   9.129   10.393      0.446    0.378       2.0   
B_10       10.335  0.219  10.116   10.554      0.154    0.131       2.0   
B_11        9.204  0.149   9.055    9.353      0.105    0.089       2.0   
```

Predictions for game: 4334:4340

```
[9.703227825416833e-179, 6.96849461631658e-179, 5.166802540962336e-179, 3.684262117916339e-179, 1.9973543953216854e-179, 1.2194235419479817e-179]
[0 0 0 0 1]
```
# Day 3: Obtain model results

## Key terms

- **Logistic regression** = binary outcome (two possible) model, models probability that given observation belongs to one category
- **Weakly informative priors** = data generated is much more extreme than we would expect from domain knowledge

## Running notes

8:28 p.m.
- Note: the only times our model got a losing point right
	- Perform analysis on these numbers – note that only when significantly lower Elo and serve
	- To do: check endgame points vs earlier points
	- Sensitivity analysis for different points, especially deuce points

```
fatigue_stdz	elo_stdz	serve_stdz	y_true	y_preds
3149	0.024465	-0.610199	-0.674419	0	0
3131	0.048064	-0.610199	-0.534884	0	0
4844	0.048647	-1.000000	-0.604651	0	0
4898	-0.018276	-1.000000	-0.558140	0	0
4696	-0.001492	-1.000000	-0.511628	0	0
4372	-0.151366	-0.747791	-0.744186	0	0
287	-0.013244	-0.833880	-0.534884	0	0
1828	-0.024505	-0.695784	-0.511628	0	0
```

- Chosen server wins 2 has
```
Accuracy: 0.875
Precision: 0.8333333333333334
Recall: 1.0
F1-score: 0.9090909090909091
```

6:24 p.m.
- Simulating the posterior predictive model form scratch
	- For each of 2000 plausible pairs of coefficients, we calculate log of the odds of rain, then transform the log into probability. Finally, from 20,000 probability values, we simulate a Bernoulli outcome.

4:22 p.m.
- Notes for writing:
	- Attribute selection – we think about possible sources of heterogeneity, both differences between score situations and differences between players

3:16 p.m.
- For flow chart
	- Use colors to indicate which player is serving
	- Use raw probabilities as labels
- Next steps: performing sensitivity analysis
	- [ ] Update distribution graphs with actual values of mean and standard deviation

9:00 a.m.
- Review of yesterday:
	- Got very frustrated towards the end!
	- Left off working on prior predictive checks
- Game plan for today:
	- Spend ~1 hour on the prior predictive checks – understanding and encoding them
# Day 1: Model planning

## Problem notes

- Need to define momentum and swing in momentum
- What does “differential in past match momentum swings” even mean?
	- Magnitude

## Ideas

- Markov chain for modeling flow of game *as points occur*
	- Need to identify which player is performing better at a given time in the match and *now much better* – post hoc analysis?
	- States could be the different point possibilities seen in [[20240201-MCM#Article [Modeling a Game of Tennis](https //towardsdatascience.com/building-a-tennis-match-simulator-in-python-3add9af6bebe)]] flow chart
		- Initial state was drawn from some kind of prior multinomial distribution (optional)
		- Draw next state based on MN distribution
	- Transition matrix
	- Problem: [Markov Chain will not capture the role of momentum](https://towardsdatascience.com/markov-chain-models-in-sports-7cb907a6c52f#:~:text=In%20Markov%20chain%20terminology%2C%20each,such%20states%2C%20shown%20as%20circles.)
	- Multiple levels of chains: https://www.sciencedirect.com/science/article/pii/S0169207022000073#sec4
- Use single model for identifying “favoring” and swings, and capturing overall flow of play as points occur
- Bayesian *updating* for transition probabilities, define model (e.g., regression) to connect factors to probabilities, Bayesian *estimation* of the parameters that represent the effects of factors 
- Final thoughts:
	- Fit a separate chain to each game in the match
	- Model every transition with a Bernoulli distribution, sample from beta distribution
	- To do: use separate transition matrices for each player?

```
import numpy as np
from scipy.stats import beta

class TennisMarkovChainWithServe:
    def __init__(self, states):
        self.states = states
        self.num_states = len(states)
        self.transition_matrices = {player: np.ones((self.num_states, self.num_states)) for player in ["playerA", "playerB"]}
        self.prior_alpha = {player: np.ones((self.num_states, self.num_states)) for player in ["playerA", "playerB"]}
        self.posterior_distributions = {player: None for player in ["playerA", "playerB"]}

    def update_transition_matrix(self, transitions, player):
        for transition in transitions:
            from_state, to_state, serving_player = transition
            if serving_player == player:
                from_index = self.states.index(from_state)
                to_index = self.states.index(to_state)
                self.transition_matrices[player][from_index, to_index] += 1

    def fit(self, transitions, player):
        self.update_transition_matrix(transitions, player)
        posterior_alphas = self.transition_matrices[player] + self.prior_alpha[player]
        self.posterior_distributions[player] = beta(posterior_alphas[0], posterior_alphas[1])

    def predict(self, from_state, player, num_samples=1):
        from_index = self.states.index(from_state)
        posterior_samples = self.posterior_distributions[player].rvs(num_samples)
        return self.states[np.argmax(posterior_samples[from_index])]

# Example usage
if __name__ == "__main__":
    states = ["0-0", "15-0", "30-0", "40-0", "0-15", "15-15", "30-15", "40-15", "0-30", "15-30", "30-30", "40-30", "0-40", "15-40", "30-40", "40-40"]
    
    # Example data from real matches
    real_match_transitions = [("0-0", "15-0", "playerA"), ("15-0", "30-0", "playerA"), ("30-0", "40-0", "playerA"), ("40-0", "Game", "playerA"),
                              ("15-0", "15-15", "playerB"), ("15-15", "30-15", "playerB"), ("30-15", "40-15", "playerB"), ("40-15", "Game", "playerB")]

    tennis_model = TennisMarkovChainWithServe(states)

    # Fit the model to the example transitions from real matches for each player
    for transition in real_match_transitions:
        tennis_model.fit([transition], transition[2])

    # Predict next state given current state and player
    current_state = "30-0"
    player = "playerA"
    next_state = tennis_model.predict(current_state, player)
    print(f"Given current state {current_state} and serving player {player}, predicted next state is {next_state}")

```


## Literature review

#### Video | [Understanding Momentum in Tennis](https://youtu.be/TDfBzQShh00?si=d4nKDH0OSykrXrV9)
- Momentum has psychological and physical effects that determine the “direction” of a tennis match
	- Players “in ascendency” have momentum
- Momentum can be unrelated to the actual score
- Momentum varies between points and sets
- Momentum can change based on anything
	- Can be visible in a match chart, or “imperceptible”
- *Momentum* book – 5 distinct phases and recommendations
	- Totally against player
	- Turning against player
	- Neutral – should play primary patterns
	- In favor
	- Totally with a player – note the player would change tactics
	- **Takeaway:** one way to discretize momentum from a continuous quantity?
- Quantifying momentum
	- Weight important points or “clutch moments”
	- Assign score to consistency, accounting for who served
- **Momentum score** affects state of mind – what are the tendencies for the specific player in a given situation?
- “Play the **momentum score**”
	- Example: tied results show better momentum for the team that was down
	- Travis & Stother, *The Art of Winning*: players statistically win twice as many points when down
- Charting momentum and understanding reactions for individual players, can show how and why for game outcomes!

#### Article | [Capturing Momentum in Tennis](https://theanalyst.com/na/2022/03/capturing-momentum-in-tennis/)

- Paper: “Live Counter-Factual Analysis in Women’s Tennis using Automatic Key-Moment Detection”
	- Use [counter-factual analysis](https://christophm.github.io/interpretable-ml-book/counterfactual.html) – interpretable technique that explains predictions of individual events
	- Impact of a play on the overall outcome
	- Highlight key moments in a match using “leverage,” “clutch,” and “momentum” metrics
- **Leverage** = for each point, a measurement of how the *next* point changes the player’s match win probability; shows the importance of a single point to the final outcome of a tennis match
	- First, use court type, current match state, in-match statistics, and pre-game odds to predict the probability of a player winning the next point
	- Next, predict the winner of the game
	- Next, predict the winner of the set
	- Next, predict the winner of the match
- **Momentum** = a value that shows which player is “in control”
	- Computed as a weighted moving average of **leverage** gained
- Plot the win probability
- **Takeaway:**
	- Need to define swing in momentum; article says switching from one to another by magnitude of 3% or more

#### Article | [Modeling a Game of Tennis](https://towardsdatascience.com/building-a-tennis-match-simulator-in-python-3add9af6bebe)

![[Pasted image 20240201162449.png]]
- Flow chart for the progression of a game of tennis
	- Note looping behavior in “deuce” situation
- Points are yes/no (Bernoulli), so entire collection can follow a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)
	- Entire tennis game can be reduced in a series of independent points with a fixed probability that the server will win each

#### Other notes
- Code snippet: Bayesian estimation for the discrete probability distribution of a transition matrix

```
Certainly! Let's consider a simple example where we model the flow of play in a tennis game using a Markov chain. The states represent different score combinations in the game. For simplicity, let's consider only the scores in a game, where one player serves. The possible states could be combinations of scores like (0-0), (15-0), (30-0), (40-0), (0-15), (15-15), ..., (40-40), etc.

We want to estimate the transition probabilities between these states using Bayesian estimation. Here's how we could proceed:

1. **Define Prior Distributions**: We start by specifying prior distributions for the transition probabilities. For simplicity, we could assume a uniform prior or incorporate domain knowledge if available.
    
2. **Collect Data**: We observe several tennis matches and record the transitions between different score combinations.
    
3. **Likelihood Function**: We define a likelihood function that represents the probability of observing the transitions given the transition probabilities in the Markov chain.
    
4. **Calculate Posterior Distribution**: Using Bayes' theorem, we update our prior distributions with the observed data to obtain the posterior distributions of the transition probabilities.
    
5. **Inference**: We perform inference on the posterior distributions to estimate the transition probabilities. This could involve calculating posterior means, medians, or credible intervals.
    
6. **Validation and Iteration**: We validate our estimates using various methods, such as cross-validation or predictive checks, and iterate if necessary.
    

For instance, suppose we observe that after (30-15), the probability of transitioning to (40-15) is high, but transitioning to (30-30) is rare. Bayesian estimation can help us quantify these transition probabilities based on observed data, and we can update our estimates as we gather more data or refine our model.

The Bayesian approach allows us to incorporate prior knowledge, update estimates as new data becomes available, and quantify uncertainty in our estimates, making it a useful tool for estimating transition probabilities in a Markov chain model of tennis gameplay.
```

```
import numpy as np
from scipy.stats import dirichlet

class TennisMarkovChain:
    def __init__(self, states):
        self.states = states
        self.num_states = len(states)
        self.transition_matrix = np.ones((self.num_states, self.num_states)) # Initialize with uniform prior
        self.prior_alpha = np.ones((self.num_states, self.num_states)) # Prior hyperparameter for Dirichlet distribution

    def update_transition_matrix(self, transitions):
        # Update transition counts
        for transition in transitions:
            from_state, to_state = transition
            from_index = self.states.index(from_state)
            to_index = self.states.index(to_state)
            self.transition_matrix[from_index, to_index] += 1

    def fit(self, transitions):
        # Update transition counts
        self.update_transition_matrix(transitions)
        
        # Update posterior distribution using Dirichlet distribution
        posterior_alphas = self.transition_matrix + self.prior_alpha
        self.posterior_distributions = [dirichlet(alpha) for alpha in posterior_alphas]

    def predict(self, from_state, num_samples=1):
        from_index = self.states.index(from_state)
        posterior_samples = np.array([distribution.rvs(num_samples)[0] for distribution in self.posterior_distributions])
        return self.states[np.argmax(posterior_samples[from_index])]

# Example usage
if __name__ == "__main__":
    # Define states (score combinations)
    states = ["0-0", "15-0", "30-0", "40-0", "0-15", "15-15", "30-15", "40-15", "0-30", "15-30", "30-30", "40-30", "0-40", "15-40", "30-40", "40-40"]

    # Define example transitions
    transitions = [("0-0", "15-0"), ("15-0", "30-0"), ("30-0", "40-0"), ("40-0", "Game"), ("15-0", "15-15"), ("15-15", "30-15"), ("30-15", "40-15"), ("40-15", "Game")]

    # Create Markov chain model
    tennis_model = TennisMarkovChain(states)

    # Fit the model to the example transitions
    tennis_model.fit(transitions)

    # Predict next state given current state
    current_state = "30-0"
    next_state = tennis_model.predict(current_state)
    print(f"Given current state {current_state}, predicted next state is {next_state}")

```

- Dirichlet distribution to sample probabilities from
	- Rows of the transition matrix are discrete probability distributions
	- Conjugate to the multinomial distribution which models the transitions themselves
- Sample Markov Chain code with serving player attribute:
```
import numpy as np
from scipy.stats import beta

class TennisMarkovChainWithServe:
    def __init__(self, states):
        self.states = states
        self.num_states = len(states)
        self.transition_matrix = np.ones((self.num_states, self.num_states))  # Initialize with uniform prior
        self.prior_alpha = np.ones((self.num_states, self.num_states))  # Prior hyperparameter for Dirichlet distribution
        self.posterior_distribution = None

    def update_transition_matrix(self, transitions):
        for transition in transitions:
            from_state, to_state = transition
            from_index = self.states.index(from_state)
            to_index = self.states.index(to_state)
            self.transition_matrix[from_index, to_index] += 1

    def fit(self, transitions):
        # Update transition counts
        self.update_transition_matrix(transitions)
        
        # Update posterior distribution using Beta distribution
        posterior_alphas = self.transition_matrix + self.prior_alpha
        self.posterior_distribution = beta(posterior_alphas[0], posterior_alphas[1])

    def predict(self, from_state, num_samples=1):
        from_index = self.states.index(from_state)
        posterior_samples = self.posterior_distribution.rvs(num_samples)
        return self.states[np.argmax(posterior_samples[from_index])]

# Example usage
if __name__ == "__main__":
    states = ["0-0", "15-0", "30-0", "40-0", "0-15", "15-15", "30-15", "40-15", "0-30", "15-30", "30-30", "40-30", "0-40", "15-40", "30-40", "40-40"]
    transitions = [("0-0", "15-0"), ("15-0", "30-0"), ("30-0", "40-0"), ("40-0", "Game"), ("15-0", "15-15"), ("15-15", "30-15"), ("30-15", "40-15"), ("40-15", "Game")]

    tennis_model = TennisMarkovChainWithServe(states)

    # Fit the model to the example transitions
    tennis_model.fit(transitions)

    # Predict next state given current state
    current_state = "30-0"
    next_state = tennis_model.predict(current_state)
    print(f"Given current state {current_state}, predicted next state is {next_state}")

```