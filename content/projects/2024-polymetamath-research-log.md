---
lastmod: 2024-08-15T09:38:49-07:00
tags:
  - workspace
---
 
aliases:
  - SFI UCR
  - undergraduate complexity research
  - Santa Fe Institute
tags:
  - workspace
publish: 
date: 2024-06-05 11:35
lastmod: 2024-08-09T00:25:07-06:00
 

DOOR CODE: 5891\*
# Content

**Overview**
- [[Polymath projects]]
- [[ucr-final-abstract]]

**Literature and related theory**
- [[A cognitive taxonomy for mathematical collaboration]]
- [[Dialogical reasoning]]
- [[Cognitive mechanisms of scientific discovery]]
- [[Collaborative problem-solving]]

**Methods**
- [[Methods for PolyMetaMath]]
- [[Topic modeling with MALLET]]
- [[Axioms of Shannon entropy]]
- [[Coarse-graining]]

**Fleeting notes and workspaces**
- [[20240802-semantic-topic-hierarchy-specs]]
- [[20240803-cognitive-topic-hierarchy-specs]]

---

# Fall 2024 log

## August

2024-08-14

- Meeting with Simon and Marina
	- Target Cogsci 2024 conference with diversity research, and possible paper
	- Work on “cognitive moves” in the long term

 ---
# Santa Fe Institute REU
## Week 1

#### Jun 5 to Jun 7 – One-on-one mentor meetings

[[2024-06-05]]

- Goal is to decide on a project that establishes a link between Andrew, Marina, hopefully including mentorship from Melanie Mitchell
- Current ideas: using Marina’s computational methods for mathematics, or investigating local maximums
- Next steps: reading a lot of papers! Focus on those by SFI authors – Marina, Wolpert, and Simon DeDeo

[[2024-06-06]]

- Agent-based modeling of math epistemology
- Determining features of proofs that lead to belief or understanding
- Next steps: finish reading DeDeo and Wolpert papers, try to establish connection with DeDeo?
	- [[2023-viteri-dedeo-epistemic-phase-transitions]]
	- [[2-SOMEDAY/2023-wolpert-kinney-stochastic-model-of-math-and-science]]

[[2024-06-07]]

- Recommendations from Chris Kempes
	- Read DeDeo, Wolpert & Kinney papers
	- Reach out to James, tracing spread of physical ideas
- Next steps: 
	- Run ideas by mentors for advice (Kempes and Marina, mainly)
	- Look for ways to contact DeDeo or anyone familiar with work
	- Schedule meeting with James

[[2024-06-08]]

- Emailed Kempes, Marina, and James about both potential projects
- Began closer reading of [[2-SOMEDAY/2023-wolpert-kinney-stochastic-model-of-math-and-science]]
- Next steps: continue closer reading of [[2-SOMEDAY/2023-wolpert-kinney-stochastic-model-of-math-and-science|Wolpert & Kinney (2023)]], possibly [[2023-viteri-dedeo-epistemic-phase-transitions|Viteri & DeDeo (2023)]] such that I can have a discussion about it

[[2024-06-08]]

- Emailed Kempes, Marina, and James about both potential projects
- Began closer reading of [[2-SOMEDAY/2023-wolpert-kinney-stochastic-model-of-math-and-science]]
- Next steps: continue closer reading of [[2-SOMEDAY/2023-wolpert-kinney-stochastic-model-of-math-and-science|Wolpert & Kinney (2023)]], possibly [[2023-viteri-dedeo-epistemic-phase-transitions|Viteri & DeDeo (2023)]] such that I can have a discussion about it

[[2024-06-09]]
- Preparatory materials for meeting with James & Andrew
	- Paper describing [pace of innovation using patterns in cities](<- [ ] [Paper describing pace of innovation measured by patterns in cities](https://www.sciencedirect.com/science/article/abs/pii/S0048733306001661?casa_token=gd1fiv_V3BIAAAAA:UsxFzTUpkBMiw5ie3S0-EbVZWqcM8beVIlvuf488qzIgfDwshBz7rMDlC-oh2FbHAjruikTk)
	- [Science innovation plots](https://docs.google.com/presentation/d/1IbLvROW-tNpxe-McTcWlHdJFC5OJEdx2rRxBeIIUL0s/edit#slide=id.g2e455f8113e_0_0)
	- Binary choice models – [discrete choice with social interactions (2022)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0267083), [binary choice (2022)](https://iopscience.iop.org/article/10.1088/2632-072X/ac8c78/pdf)
	- Formal models of social disagreement in science (Seselja, 2022)

 

## Week 2

#### Jun 10 to Jun 11 – Write project abstract

[[2024-06-10]]

- Meeting with James
	- Possible to collect the relevant data?
		- Proof ideas: Pythagorean theorem, Hopf’s Umlaufsatz
		- Scraping answers and discussions on Math Stack Exchange for social consensus on proofs
		- “Biographical” or qualitative descriptions of the proving process (modeling piecewise?)
- Next steps
	- Prepare for tomorrow’s meeting with Chris Kempes
	- Prepare for potential meeting with DeDeo

[[2024-06-11]]

- Met with Simon DeDeo, decided on project, submitted initial project summary
- Next steps
	- Encode claims in Polymath paper
	- Continue literature review, particularly sources recommended by DeDeo
		- Michael Nielsen, *Reinventing Discovery*
		- Sid Redner’s power-law exponent
		- DeDeo AMS paper
		- Catherine Novales Duluth, *Dialogical Roots of Deduction*, general thoughts

#### Jun 11 to Jun 14 – Research days

[[2024-06-12]]

- Polymath paper – exposition gives guidance on *how to think*, very little is actually captured in the statements of theorems themselves
	- Sections 4, 5, and 9 are given as “sketches” instead of formal arguments – reader is meant to come away with confidence anyway
	- Would be interesting to look at comments for why particular approaches to *conceptualization* were used
- Next steps
	- Watch *Intro Complexity* network videos
	- Continue literature review on philosophy of math, group reasoning; finish papers recommended by DeDeo
	- Look into MG’s work on collective decision-making
	- Look at degree distribution, compute power-law exponent
	- Ask Alex for network help

[[2024-06-13]]

- Watched *Intro Complexity* section on networks – [[20240613-lecture-networks]]
- Performed basic analysis on graph of explicitly cited claims, with all relations
	- Found $\alpha = 2.134, \ \sigma=0.242$; 27 nodes and 28 links 
- Next steps
	- Understand relevance of Redner “generative assembly model” paper – the communal reasoning composition as a network that grows slowly over time? Where to draw the distinction between a well-formed proof “piece” that is added to the overall argument?
	- Talk to Mirta Galesic about collective problem-solving – how do these sorts of things scale? Categorically different situation when goal is this cohesive?
	- Look into sources of comments for Polymath projects – [polymath1](https://gowers.wordpress.com/category/polymath1/)

[[2024-06-14]]

- Project meeting day
	- Continue literature review
	- Look at blog posts, start thinking of a taxonomy of social interactions (c.f. the taxonomy of scientific social interactions from Marina’s work?)
- Next steps
	- Finish book on dialogical proving
	- Start looking at blog posts and comments

 
## Week 3

[[2024-06-17]]

- More literature review
	- [What makes Individual I's a Collective We; Coordination mechanisms & costs](https://arxiv.org/abs/2306.02113#:~:text=What%20makes%20Individual%20I's%20a%20Collective%20We%3B%20Coordination%20mechanisms%20%26%20costs,-Jisung%20Yoon%2C%20Chris&text=Collectives%20must%20coordinate%20and%20regulate,the%20sum%20of%20individual%20abilities.)
	- [The Polymath Project: Lessons from a Successful Online Collaboration in Mathematics](https://www.cs.cmu.edu/~jcransh/papers/cranshaw_kittur.pdf)
	- [Internet Collaboration on Extremely Difficult Problems: Research versus Olympiad Questions on the Polymath Site](https://www.cs.cornell.edu/home/kleinber/www16-polymath.pdf)
	- [Toward a Comparative Cognitive History: Archimedes and D. H. J. Polymath](https://arxiv.org/abs/1204.3534)

[[2024-06-18]]

- Tutorial takeaways – flash talks
	- Include project plans for feedback
	- Humor is good, as well as intentionally re-engaging the audience at topical transitions
	- Keep a forward momentum, try not to flip back to previous slides
	- Tell an overarching story
	- To practice or not to practice?
- Meeting with Simon & Marina
	- Ideas
		- Hidden Markov model of reply threads – comment emits one argument pattern, replies emit another; possibility that there are 6 argument patterns but 3 hidden states, etc; more general look at how argumentation happens
		- Identifying how confidence is established, e.g. presence of enthymematic gaps
	- Next steps
		- Scrape posts for comment data
		- Implement Na clustering technique
		- Continue literature review to identify significant suggested reasoning processes (e.g., Type 1 vs. Type 2 methods of establishing confidence, [[Deductive dialogues are Prover-Skeptic games, after Dutilh Novaes|Prover-Skeptic]] framework) and “argument” types (e.g., counterexamples, probabilistic reasoning)

[[2024-06-19]]

- Tutorial takeaways – organization and project management
	- Storage
		- PCloud for secure cloud storage
	- Preparing code for publication
		- File structure: under a “repos” folder, keep repositories per project
		- README file can be done any time during process
		- Example: subfolders for figure-producing scripts, MatLab functions, figures that are automatically generated by scripts, “schematic” or non-automatic figures
		- Figshare – figures and data repository, useful for large datasets
		- Archive to DOI
- Scraped data from all threads related to Polymath1
	- Data for each comment contains: author, date and time string, comment content; date and time stored in correct type
- Suggestions for potential analysis
	- ? – vectorize and examine vector space movement (?)
	- Sara Walker – compare number of comments that are “memory” to what is truly novel
		- What counts as novel, if mathematics is deductively built?
- Next steps
	- Start parsing out reply chain – account for comment replies in dataset
	- Get suggestions for how to interpret comment content
	- Share code with Simon and Marina over GitHub

[[2024-06-20]]

- Added more information for each comment
	- Custom ID, e.g., `G300-1000` for Gowers comment 1000 on post 300
	- References to parent comment if in blog “reply” structure (`in-reply-to`, `in-reply-to-href`)
	- Links to comment itself, and author if available

[[2024-06-21]]

- Contemplating assembly theory perspective and math – mostly reading day (didn’t feel very productive, however)
- Meeting with Marina & Simon – see daily note
	- Topical modeling how-to
- Next steps
	- Make a text file for all relevant blog links
	- Finish collecting *clean* data for as many projects as possible

 
## Week 4 - Flash talks


[[2024-06-24]]

- Saving URLs
	- For consistency, only saved “research” threads that were officially tagged under the project name for each blog
	- Omitted comments to published papers for now, since the interaction is not fully captured in the comments (but can add)?
- Tentatively scraped all comments, reasons for inconsistency with Kloumann paper?
- Next steps
	- Add project id column
	- Include up/down vote data

[[2024-06-25]]

- Meeting with Simon and Marina
	- Clean comments for topic modeling – remove LaTeX for now, change to all lowercase, possibly standardize tenses?
	- Get word counts and remove top words
	- Start “quick and dirty” topic modeling
- Slack discussion – potential plots 
	- “The distribution of the length of comments in words; this might tell us a little bit about the “norms” of the system. My guess is that it will look rather Poissonian (with a characteristic length that indicates a target), rather than exponential (implying a constant “probability of stopping”) or power law (indicating more complex processes beloved of the Santa Fe Institute).”
	- “A scatter plot of “number of non-Latex words” versus “number of Latex equations”, that might separate out “business posts” from more discursive ones”
- Debugging for web scraping
	- Fixed replies for Polymath blog
	- Changed LaTeX to print alt text, bracketed by $s
- Starting topic modeling
	- Downloaded necessary packages – will probably have to work from command line
	- Consolidated and cleaned all comment *content* – did not remove single characters or numbers
- Next steps
	- Decide which of the most common words to remove from content – check if I should use a single “document”
	- Run quick start topic model using the command line

[[2024-06-27]]

- Notes from giving flash talk
	- Tamyra Walker – look into ethnomathematics, consider how “mathematical reasoning” might be qualified
	- Cris Moore – tracking math concepts; extend to comparisons to final proof
- Topic modeling
	- Tried with 10 and 20 topics
- Next steps
	- Talk to Tamyra (`tamyrawalker@gmail.com`) about quantitative social science, topic modeling, ethnomathematics
	- Get access to “fast machines” via ssh
		- `bnnyng@ganesha.lan.cmu.edu`, `12-Pr48`
		- [http://www.linuxproblem.org/art_9.html](http://www.linuxproblem.org/art_9.html)
		- https://computing.cs.cmu.edu/security/security-ssh

[[2024-06-28]]

- Meeting with Simon & Marina
	- Current stage is “playing around”; aim to get “cognitive move” word list
	- Interviewing real people about how they conduct math, take inspiration from [[2024-dedeo-alephzero-and-mathematical-experience]]
- Topic modeling
	- Redid model with a large number of topics (100)
	- Filtered comments for words not shared across projects; in particular, words not used at least three times in all *main* Polymaths
- Next steps
	- Try a smaller number of topics (5-10)
	- Get top comment examples for each topic

 
## Week 5

[[2024-07-01]]

- Retrieved top comments for 10 topics with shared words only
	- Possibly too constrained? Instead, try words that appear in 50 percent of Polymaths, standardizing words
- Made general plots for data
	- Distributions of comment lengths (number of words)
	- Number of words vs. number of TeX equations
- Next steps
	- Examine example comments and get a sense of topic classifications


[[2024-07-02]]
- Meeting with Simon and Marina
	- Make final word filtered list, then train topic model again
	- Adjust hyperparameters during training
	- Aim for smaller number of categories to be more interpretable—look for log-likelihood asymptote
- Reading recommendations from Simon
	- “The Birth of a Theorem”
	- Rota, “Indiscrete Thoughts”
	- Ruben Hirsch
- Finalizing word list
	- Inspecting top 15 comments from each previously assigned topic, adding words to keep based on original text and *removing* words based on filtered content; general heuristic is to remove words highly associated with categories that seem to be about one math topic
- Next steps
	- Finalize word list
	- Retrain topics, adjusting hyperparameters and looking at log-likelihood asymptote for optimal numbers
	- Create Overleaf document and appendix with examples for each topic
	- Try working with bi-grams

[[2024-07-03]]
- Finalizing word list – see daily note for heuristics
	- Added bi-grams
- Try MALLET modeling with 5 and 10 topics
- Next steps
	- Put results in an Overleaf document

[[2024-07-05]]

- Running anchored CorEx models – see daily note
- Meeting with Simon and Marina
	- Further word filtering
	- Use domain knowledge to come up with “anchor words”
	- Verify by running unsupervised model on the documents with top 100 words from each topic, etc.
- Next steps
	- Come up with list of questions to ask mathematicians
	- Data preprocessing for CorEx – use higher minimum bi-gram ratio

 
## Week 6

[[2024-07-08]]
- Trying contextual/combined models
- Next steps
	- Solidify topics and decide on metrics for evaluation
	- Clarify next directions for research

[[2024-07-09]]

- Meeting with Simon and Marina
	- Finalize word “inclusion” list and run unsupervised model
	- Can perform analyses using both supervised and unsupervised results
- Next steps
	- Run LDA with “inclusion” word lists and select best model (based on interpretability and loss?)

[[2024-07-10]]

- Refined final word inclusion list
- Attempting different unsupervised models, made pipeline to view top comments and basic distribution plots; final topic model still undecided
- New idea: exploration/exploitation by determining distance between semantic topics
- Next steps
	- Build pipeline for analyzing exploration/exploitation within a single project
	- Decide on final cognitive topic model

[[2024-07-11]]

- Initial cleaning for semantic words dataset, wrote function for measuring KL divergence between adjacent distributions
- Went through readings on neural topic models and embeddings
- Next steps
	- Implement embedded neural topic model
	- Label topics for example models on Slack

[[2024-07-12]]

- Made neural topic model with word embeddings
- Decided on 10 topics with Simon and Marina
- Next steps
	- Make a quiz for classifying comments into topics
	- Get more examples for topics 5 and 8

2024-07-13
- Got most distinct comments for MALLET topics
- Scraped main blog posts to make larger corpus for “semantic” topics

 
## Week 7

[[2024-07-15]]

- Cleaning data to use in embedded topic model
- Implemented and tested HyperETM – see daily note for code
- Next steps
	- Look into paper from Simon on potential analysis methods
	- Complete topics “quiz”

[[2024-07-16]]

- Top2Vec fit for semantic (math) topics, tried hidden Markov model (see daily note for code)
- Meeting with Simon and Marina
	- Group comments and take “diversity” of semantic topics, as well as diversity of commentary (see daily note for message from Simon)
	- Jensen’s Shannon distance – “how different are the mixtures?”
	- Entropy of averaged distribution over comment group – “how spread out over all topics is this group?”; potentially misses the diversity aspect, e.g., the pairs \[1.0, 0.0] and \[0.5, 0.5] will have the same entropy, yet intuitively have different diversities
- Next steps
	- Compute diversity measures for trained Top2Vec model

[[2024-07-17]]

- Finished collecting Wikipedia corpus
- Training Top2Vec and embedded models on new corpus – failed to save
- Added cosine similarity (averaged over a group) as a new measure of diversity
- Next steps
	- Fix computation of JSD
	- Embedded model using *document vectors*, rather than vocab vectors

2024-07-21
- Retrained both embedding and Top2Vec
- New adjustments to Top2Vec corpus
	- Remove blank comments and posts
	- Remove meta posts and comments

 
## Week 8

[[2024-07-22]]

- Tutorial – Effective strategies for reading (Daniel)
	- Determine why you’re reading a paper, figure out most relevant sections
	- Parse which parts are argumentative/rhetorical and which are “reporting”
- Computed diversity measures for Top2Vec topics on extended corpus
	- Using cosine similarity for creating distributions – add minimum (negative) value, then renormalize to sum to 1
- Next steps
	- Examine critical points
	- Look at diversity of commentary
	- Read more literature on diversity
	- Figure out optimal number of topics

[[2024-07-23]]

- Meeting with Simon
	- See French Revolution paper for MALLET preprocessing methods
	- Turning “knobs” – group size, grouping by time, number of topics, using all project comments simultaneously, word minimums
		- Divide each project into 10 eras, then take the average JSD in subgroups of varying sizes (e.g., pairs, groups of 5, groups of 10); interesting to see if there are any “characteristic” group sizes
		- Get standard error of each group as well
		- Optimal word minimum can be determined by plotting entropy against comment length
- Computed diversity for Polymath1 using P1 data only *and* all project data
	- Modeling with MALLET – 10, 20, and 100 topics for basics. Tried 20 and 50 topics as well for all project corpus
	- Divide each project into 10 groups, then play with subgroup sizes. The JSD is computed for each subgroup, and the diversity for each of the 10 main groups is computed as the average JSD of all its subgroups
	- Subgroups can be made with discrete or sliding windows
- Next steps
	- Estimate error bars for sliding subgroups (“standard error is not quite valid”)
	- Plot fraction of comments in each bin that are replies

[[2024-07-24]]

- “Knobs” to decide: cut-off comment length, changes in diversity curve as a function of subgroup size, dataset to use
- Results from plotting log(comment length) vs. entropy, total # of topics vs. correlation
	- Estimating topic compositions is difficult for large numbers of topics and short comments (low comment lengths)
	- For Polymath1, diversity is low in the first 20 percent of comments, then jumps up and remains relatively stable in the rest of the data; this is invariant to choice of subgroup size, number of topics, or building the topics with all project/P1-only data
- Generated diversity plots (Jensen-Shannon Divergence) for all projects
	- Diversity changes over time, but no overall patterns
	- Compared with topics based on cognitive “inclusion” data
- Computed **surprise** and **resonance**
- Next steps
	- Examine model outputs – top comments, distinct comments, “junk” topics
	- ~~Try to find patterns using other measures – surprise and novelty/transience (see French Revolution paper)~~
	- Play with clustering cognitive topics
	- Plot fraction of comments that are replies
	- ~~Measure novelty and transience for top Polymath contributors~~

[[2024-07-25]]

- Testing different topic models
	- Tried topics based on project-specific data only, no meaningful results
- Revisiting literature, starting “big picture” thinking again
- Next steps
	- For top contributors, compare KL between self and other comments
	- (Potential) Look for heuristics based on Gowers’ paper on mathematical belief

[[2024-07-26]]

- Organizing ideas about “cognitive moves”: [[A cognitive taxonomy for mathematical collaboration]]
- Meeting with Simon
- Next steps
	- Add error bars for novelty scatterplots
	- Work on refining cognitive topics, augmenting word lists
	- Re-run analyses on singular project

## Week 9

2024-07-29
- Tried modeling with anchored CorEx – led to high correlations between comment length and entropy

[[2024-07-30]]

- Meeting with Simon and Marina
	- Potentially use LLMs to describe topics after getting top words
	- Develop a distance matrix for the dendrogram – JSD is a natural measure
	- Use circular histogram to look at the topic range of each group
- Created semantic “inclusion” word list for Polymath1
- Implemented hierarchical clustering and dendrogram plots
- Next steps
	- ~~Measure diversity with hierarchical clustering~~

[[2024-07-31]]

- Finished semantic word list for Gowers’ posts in Polymath1
- Experimented with different hierarchy quantities
- Ran same analysis on current cognitive word list
- Next steps
	- ~~Compute difference between fine and coarse-grained diversities – 20 and 6~~
	- Create “unwrapped”/linear dendrograms
	- Coarse-grain cognitive topics from 30 original topics
	- ~~Compute hierarchy with word-topic matrix~~

[[2024-08-01]]
- Made public repository for Polymath data
- Recomputed hierarchies using word-topic distributions, ran same analyses: divergence for various levels of coarse-graining *and* differences between levels
- Spent far too long messing with dendrogram plot
- Next steps
	- Outline main narrative of talk
	- Draft talk slides
	- Look into plotting software – Tabeleau, D3.js

[[2024-08-02]]

- Meeting with Simon and Marina
	- Plan for the talk
		- Give general background for theories that the results most directly relate to (e.g., diversity in scientific discovery, dialogical reasoning); recall that the interest of the project is applying this investigation to mathematical collaboration!
		- Begin with fine-grained analysis (i.e., example comments, labels/keys for topics)
		- “Zoom out” to plot cognitive moves across all projects
	- Follow-up steps taken
		- Looked into outlier (low diversity) comment group – made methods to get comments from group
		- Checked radar plot computations
- Prepared briefly for final talk ([[20240802-a-potential-talk-narrative]])
- Evaluated diagnostics file for original 30-topic model 
- Used hierarchical reduction to get topics from 5 to 29 ([[20240802-semantic-topic-hierarchy-specs]])
	- Implemented “topic diversity” measure from [Dieng et al. (2019)](https://arxiv.org/abs/1907.04907)
	- Generated preliminary labels for 8-topic reduction
- Next steps
	- ~~Figure out correct computation for radar plots~~
	- ~~Look into outlier comment group with particularly low diversity (2/3/4)~~
	- Find way to incorporate “highlight” comments into visualizations
	- Complete a satisfying “cognitive moves” list and make radar plots for all projects
	- Look into references from Marina
	- Decide which topic reductions to use, get labels


[[2024-08-04]]
- Put together most talk slides
- Next steps
	- Complete background information portion of talk

## Week 10 – Final talks

2024-08-07

- Final talk preparation
	- [x] Run through slides in reverse
- Final talk slides
	- [x] Fix acknowledgement slide
	- [x] Download and refine final version
- Next steps
	- Read [“Topics in Semantic Representation”](https://psycnet.apa.org/doiLanding?doi=10.1037%2F0033-295X.114.2.211) for theory of inferring information from text

[[2024-08-08]]

- Drafting project abstract
- Future directions from Simon (via Slack)

 ---
# References

- Topic modeling
	- [Complexity Explorer course from Simon](https://www.complexityexplorer.org/courses/162-foundations-applications-of-humanities-analytics-spring-2023/segments/15615?summary)

- Topic modeling with CorEx
	- [Notebook with example](https://gist.github.com/patrickvankessel/0d5bd690910edece831dbdf32fb2fb2d)
	- [Main example with sparse matrix](https://github.com/gregversteeg/corex_topic/blob/master/corextopic/example/corex_topic_example.ipynb)

 
# Further notes


- Potential avenues of investigation
	- Does interaction change as people get more familiar with online etiquette?
	- Do people engage in Lakatos-style [[Dialogical reasoning]]?
	- Find “hidden states” of local dialogue patterns
	- Exploration-exploitation trade-offs – use semantics, possible to extract based on cognitive moves as well?
	- Effects of individual-level strategies vs. system-imposed roles (e.g., Polymath hosts)
	- Relationship between data and highlighted comments – use wiki changes as a starting point?
- Next steps
	- Find a better notion of “success” or appropriate DV

```
0. Brainstorming approaches & gaining motivation from elsewhere (e.g. external textbooks, etc)
1. Exploratory deductive: reflecting on the already proposed ideas & restating the problem/solutions & deducing the conclusions (interpersonal)
2. Computed the result
3. Deducing by contradiction
4. Open questions & possibilities
5. ?
6. Inductive, intuitions
7. Considering someone else's point, (inductive) reasoning through possibilities (interpersonal)
8. ? Examples, uncertainty, planning (interpersonal)
9. Correcting errors (interpersonal)
```

#### Topic models

**Top2Vec v1 - 243 topics**
- Data: comments and posts (each paragraph as a single document) only, removed words that were shared across more than 40 percent of documents for corpus, vocabulary also removed words that were shared across less than 5 percent

**Top2Vec v2**
- Data: comments, posts, and wiki pages (each subsection as a single document). Same word removal for comments and posts; removed words for wiki that were English stop words, shared across more than 10 percent of documents or appearing in less than 3 documents
- Removed all comments with the following words: `'metacomment', 'meta', 'post', 'discussion'`

**Word2Vec + pre-trained embeddings**
- Data: same as Top2Vec v2
- Word2Vec parameters:
```
bigram_transformer = Phrases(documents)
bigram_phraser = Phraser(bigram_transformer)
documents_bigrams = [bigram_phraser[sentence] for sentence in documents if len(sentence) > 0]
model_embedding = Word2Vec(
    sentences=documents_bigrams,
    vector_size=300,
    window=10,
    min_count=10,
    workers=4,
    sg=1,
    epochs=10
)
```

#### MALLET training pipeline

```
bnnyng@ganesha.lan.cmu.edu
12-Pr48
```

Importing data to be used by Mallet
```
bin\mallet import-file --input data\data-mallet-semantic-P1.tsv --output semantic-P1.mallet --keep-sequence 
```

```
bin\mallet train-topics --input semantic-extended.mallet --num-topics 100 --output-state semanticOutput100.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics100.txt --output-topic-keys semanticKeys100.txt 
```
