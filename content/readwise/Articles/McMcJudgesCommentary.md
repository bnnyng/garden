# McMcJudgesCommentary

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article2.74d541386bbf.png)

# Metadata
- Author: [[COMAP Inc.]]
- Full Title: McMcJudgesCommentary

- URL: https://readwise.io/reader/document_raw_content/127449600

# Highlights
- relying on some form of regression, ARIMA, or machine learning model. However, an important requirement of any prediction interval is to provide an associated confidence level or some other robust measure of uncertainty to assess the quality of the estimate ([View Highlight](https://read.readwise.io/read/01hkcfeqncc3h67sc1fe3erna3))
- The best papers provided welldefined prediction intervals and attached an effective measure of the uncertainty or level of confidence associated with it ([View Highlight](https://read.readwise.io/read/01hkcfh3sabx50hey27p1zgpw2))
- a neat heat map ([View Highlight](https://read.readwise.io/read/01hkcfm4p4m3a2wr6hxmbsjfp0))
    - Note: Creativity is rewarded!
- latter providing an excellent interpretation of the coefficients in their model, a rarity that resonates very well with judges ([View Highlight](https://read.readwise.io/read/01hkcfnq8w6r79p8knc86f67fx))
- The ideal result, as is the case with any predictive model, is to provide a prediction interval with an associated confidence level or other robust measure of uncertainty ([View Highlight](https://read.readwise.io/read/01hkcftmvaghwhh070kqs2q2jx))
- Logical and thorough analysis of the uncertainties and confidence levels associated with the predictions separated the better papers from the others. The more successful teams also achieved better results for goodness of fit to justify their models to some degree ([View Highlight](https://read.readwise.io/read/01hkcfvc5ssxgsf7h8gab1z815))
- Such a strategy of validating a predictive model is generally considered a best practice when assessing mathematical models yet scant few teams did so. ([View Highlight](https://read.readwise.io/read/01hkcfxbndr586q022t34qwrx9))
- What separated the Outstanding teams from the others was a clear explanation of the clustering process and the interpretation of their results. The better teams also supported their conclusions with specific examples that could be verified through the data accompanied by effective visualizations ([View Highlight](https://read.readwise.io/read/01hkcg0p1w44f2ab9kdhh818yy))
    - Note: Visualization important again!
- The sensitivity of a model is a good indicator of model performance,
  so it is an important feature of any modeling effort. It is also a good opportunity to assess the robustness of a model to assumptions. ([View Highlight](https://read.readwise.io/read/01hkcg4vvg218w0cva2nsrcyab))
- Any parameter that is estimated in a model is a good target for sensitivity analysis. Another possible target is the input variables themselves. ([View Highlight](https://read.readwise.io/read/01hkcg47djvaje4na8j54qrjy4))
- poorly. ([View Highlight](https://read.readwise.io/read/01hkcg3ckzw32m71eeajhs1jhv))
    - Note: What does this mean?
## New highlights added January 7, 2024 at 11:59 PM
- Judges generally assessed the executive summary and the letter to the editor to get an overview of the paper and a quick assessment of its quality before reading the rest of the paper. As those two elements form the basis of the judgesâ€™ first impression, spending extra time on them is worth the investment ([View Highlight](https://read.readwise.io/read/01hkftt0r6hd82hzxqcn4f8c51))
- it is important for each paper to identify what could have been done differently if given more time ([View Highlight](https://read.readwise.io/read/01hkfvsz1y35nmjwnpc7t3vh3x))
- It is rare to see teams attempt to interpret the resulting components in context. It is important to attach meaning to the components to adequately assess the quality of the modeling results. ([View Highlight](https://read.readwise.io/read/01hkfvrvscmegv8s22y79mmppx))
