# The Artificiality of Alignment

![rw-book-cover](https://thegradient.pub/content/images/2023/09/alignment.png)

## Metadata
- Author: [[Jessica Dai]]
- Full Title: The Artificiality of Alignment
- Category: #articles
- Document Tags: [[topic-technology-internet]] 
- URL: https://thegradient.pub/the-artificiality-of-alignment/

## Highlights
- The idea is that AI systems will inevitably surpass human-level reasoning skills, beyond “artificial general intelligence” (AGI) to “superintelligence”; that their actions will outpace our ability to comprehend them; that their existence, in the pursuit of their goals, will diminish the value of ours ([View Highlight](https://read.readwise.io/read/01hj36d740qbyte7ss71vm6wya))
    - Note: Contrast with Chiang, AIs greatest risk is being capitals executioner.
- In a recent [NYT interview](https://www.nytimes.com/2023/04/12/world/artificial-intelligence-nick-bostrom.html?searchResultPosition=2), Nick Bostrom — author of *Superintelligence* and core intellectual architect of effective altruism — defines “alignment” as *“ensur[ing] that these increasingly capable A.I. systems we build are aligned with what the people building them are seeking to achieve.”* ([View Highlight](https://read.readwise.io/read/01hj36ew9hqx4718sj1jwp88fh))
- No matter what may be publicly stated, revenue generation will always be at least a complementary objective by which OpenAI’s governance, product, and technical decisions are *structured*, even if not fully *determined* ([View Highlight](https://read.readwise.io/read/01hj36jcdwnv4hyt4tvrrx5np7))
- So OpenAI and Anthropic might be trying to conduct research, push the technical envelope, and possibly even build superintelligence, but they’re undeniably also building *products* — products that carry liability, products that need to sell, products that need to be designed such that they claim and maintain market share ([View Highlight](https://read.readwise.io/read/01hj36hpys4ytkm4bhhf76snh8))
- Paul Christiano, who previously led the alignment team at OpenAI, defines intent alignment as *“AI (A) is trying to do what Human (H) wants it to do.”* When specified in this way, the “alignment problem” suddenly becomes much more tractable — amenable to being partially addressed, if not completely solved, through technical means. ([View Highlight](https://read.readwise.io/read/01hj36nscv4dfn9eks0rp3txah))
- The key goal in this line of work is to develop a model of human preferences, and use them to improve a base “unaligned” model ([View Highlight](https://read.readwise.io/read/01hj36pzfmexddgnaq1rybda2c))
- reinforcement learning with human feedback” ([RLHF](https://openai.com/research/learning-from-human-preferences)) and its successor, “reinforcement learning with AI feedback” (RLAIF, also known as [Constitutional AI](https://www.anthropic.com/index/constitutional-ai-harmlessness-from-ai-feedback)) ([View Highlight](https://read.readwise.io/read/01hj36ts2cfs0ffbbgmq77927q))
- The “financial sidequest” sidesteps both of these issues, which captures my real concern here: the existence of financial incentives means that alignment work often turns into product development in disguise rather than actually making progress on mitigating long-term harms. ([View Highlight](https://read.readwise.io/read/01hj36x3k4js1xzy7edb9nprtc))
- As we’ve seen in online platforms, where content moderation policies are unavoidably shaped by revenue generation and therefore default to the bare minimum, the desired generality of these large models means that they are also overwhelmingly incentivized to *minimize* constraints on model behavior ([View Highlight](https://read.readwise.io/read/01hj370cc07ptzmmy4dkjrx3xk))
    - Note: There will always be someone who has less restrictions, and that product will be more successful etc
- Rather than asking, “how do we create a chatbot that *is* good?”, these techniques merely ask,  “how do we create a chatbot that *sounds* good”? ([View Highlight](https://read.readwise.io/read/01hj371vt570a8c611m08tas2v))
