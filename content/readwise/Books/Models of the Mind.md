---
lastmod: 2024-02-19T20:08:27-08:00
---
# Models of the Mind

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/reader/parsed_document_assets/125244100/-pKJr1Bw93LpkbewHozgdMvMqsWzapcH8q8xtpAYexQ-cove_EvNBvSK.jpg)

# Metadata
- Author: [[Grace Lindsay]]
- Full Title: Models of the Mind

# Highlights
- As Laplace himself wrote: ‘Probability theory is nothing but common sense reduced to calculation.’ ([View Highlight](https://read.readwise.io/read/01hppyr9ak1fsrek8cfgw4e994))
- John Anderson formally debuted a Bayesian approach to psychology, under a method he referred to as ‘rational analysis’. ([View Highlight](https://read.readwise.io/read/01hppys5xpa8r10axq03qev81n))
    - Note: Origins of online "rationalism"?
- Another open question about priors is their origin. Priors can serve as an efficient way to imprint facts about the world on our minds; but are these facts gifted to us from previous generations through our genes, or do we develop them in our own lifetimes? ([View Highlight](https://read.readwise.io/read/01hppz0z8yx0epzw1nsznq16kd))
- According to behaviourism, psychology should not be defined as the study of the mind, but rather, as the study of behaviour. ([View Highlight](https://read.readwise.io/read/01hppz59mqwcmz870g7xbazvz3))
- The mathematical conclusion this led him to is now known as the Bellman equation and the simple intuition it captures is that the best plan of action is the one in which all the steps are the best possible ones to take. ([View Highlight](https://read.readwise.io/read/01hppz7fna5y0cdm4mxbsnbtg8))
- In sequential decision-making, the different positions we can move through are called states and the steps in a plan are frequently referred to as actions. ([View Highlight](https://read.readwise.io/read/01hppza24c8z1655p2d5t8nvw0))
- Using the same recursive structure introduced earlier, Bellman defined the value of a state as the reward you get in that state plus the discounted value of the next state. ([View Highlight](https://read.readwise.io/read/01hppzakt1eng4sryfp3txrcjh))
- free energy is the difference between the brain’s predictions about the world and the actual information it receives. ([View Highlight](https://read.readwise.io/read/01hppzgyjp0d5te2616hg6jtvh))
- The free energy principle, however, is more a way of looking at the brain than a strong or specific claim about how it works. As Friston said: ‘The free energy principle is what it is—a principle … there’s not much you can do with it, unless you ask whether measurable systems conform to the principle.’ ([View Highlight](https://read.readwise.io/read/01hppzkwjch7q2fjx2jhf4g5pp))
    - Note: On the difference between a falsifiable claim and a "principle."
## New highlights added February 19, 2024 at 1:42 AM
- This outsourcing of intellect to the environment is known as ‘extended cognition’.
  Mathematics is a form of extended cognition. ([View Highlight](https://read.readwise.io/read/01hpzv526vyj1nyptrdfxghb73))
    - Note: Really resonating with meta science perspective.
- When an analogy exists between underlying mechanisms, equations serve as the embodiment of that analogy. As an invisible thread tying together disparate topics, mathematics is a means by which advances in one field can have surprising and disproportionate impacts on other, far-flung areas. ([View Highlight](https://read.readwise.io/read/01hpzv7ggrh0hsdqms65q1w6px))
    - Note: Philosophy of complex systems, SFI
- Equations force a model to be precise, complete and self-consistent, and they allow its full implications to be worked out. It is not difficult to find word models in the conclusions sections of older neuroscience papers that sound reasonable but, when expressed as mathematical models, turn out to be inconsistent and unworkable. Mathematical formulation of a model forces it to be self-consistent and, although self-consistency is not necessarily truth, self-inconsistency is certainly falsehood. ([View Highlight](https://read.readwise.io/read/01hpzvaxaemg08pgxyaq83vzrf))
    - Note: The role of mathematical modeling.
- Detail for detail’s sake is not a virtue. A map the size of the city has no good use. The art of mathematical modelling is in deciding which details matter and steadfastly ignoring those that do not. ([View Highlight](https://read.readwise.io/read/01hpzvcp25r6wjvt6tc89j7a8g))
## New highlights added February 19, 2024 at 7:32 PM
- This finding – that the size, shape or duration of an action potential emitted by these sensory neurons does not change, no matter how heavy or light the weight applied to the muscle is – Adrian referred to as the ‘all-or-nothing’ principle. ([View Highlight](https://read.readwise.io/read/01hq21e4nq0centz08z4tkqcg8))
- As Shannon says, the components of his information-sending model are ‘suitably idealised from their physical counterparts’. This is possible because, in all of these cases, the fundamental problem of communication remains the same. It is the problem of ‘reproducing at one point either exactly or approximately a message selected at another point’. ([View Highlight](https://read.readwise.io/read/01hq21hrj6rttzfvktth48nwzy))
    - Note: Similar divorce as Marr’s tri level analysis in cogsci
- To maximise a code’s entropy, each of its symbols should be used the exact same amount ([View Highlight](https://read.readwise.io/read/01hq21v5009nhn0vtwpxp7kw32))
- In general, some evidence of rate-based coding can be found in most areas of the brain ([View Highlight](https://read.readwise.io/read/01hq21zrmz3a1510fkmf6qamtp))
- a temporal code has been transformed into a spatial code: the position of the active neuron in this map carries information about the source of the sound. ([View Highlight](https://read.readwise.io/read/01hq220pdwxfc38c00krydx6b2))
- Some neurons, in some areas of the brain, under some circumstances, may be using a rate-based code. Other neurons, in other times and places, may be using a code based on the timing of spikes, or the time in between spikes, or some other code altogether. ([View Highlight](https://read.readwise.io/read/01hq2215hsjc5k5a91qa0fjeeh))
- the efficient coding hypothesis, the idea that – no matter what code the brain uses – it is always encoding information efficiently ([View Highlight](https://read.readwise.io/read/01hq221sh0n5qx5xqamvgegrf7))
    - Note: Relationship to FEP?
- Adrian called this phenomenon ‘adaptation’ and defined it as ‘a decline in excitability caused by the stimulus’. ([View Highlight](https://read.readwise.io/read/01hq227wpzh5c2858kejfwjqrz))
    - Note: A form of memory, specifically perceptual learning?
- One important one is that the brain is an information-processing machine. That is, it does not aim to merely reproduce messages sent along it, but rather to transform them into action for the animal. It is performing computations on information, not just communicating it ([View Highlight](https://read.readwise.io/read/01hq22deegsz53xr9ekmdn2mh6))
    - Note: Limitation to the notion of neural code; I guess the model is more of a historical interest atp? Is anyone still researching this seriously?
      Ah, just a metaphor.
- Network neuroscience, the name given to this practice of using the equipment of graph theory and network science to interrogate the structures of the brain ([View Highlight](https://read.readwise.io/read/01hq22k0k3g9d7drhwde9wrhvj))
- the probability of your hypothesis (‘the die is weighted’) given your evidence (the rolls you’ve seen) is proportional to the probability of your evidence given your hypothesis (the odds you’d see those rolls if the die were weighted) times the probability of your hypothesis (how likely is the die to be weighted to begin with) (see Figure 22 ([View Highlight](https://read.readwise.io/read/01hq22p09je9j9mghmbx001sz8))
- The ‘prior’ is the name given to the probability of the hypothesis – in this case the probability your friend has altered the die ([View Highlight](https://read.readwise.io/read/01hq22q79p0rb2w3pn2qb4dw46))
    - Note: Strong priors are a deciding factor.
- ‘likelihood’. It indicates how likely you’d be to see what you’ve seen if your hypothesis about the world were true. Its role in inverse probability reflects the fact that, to determine the cause of any effect, one must first know the likely effects of each cause ([View Highlight](https://read.readwise.io/read/01hq22sahayerpv2y32znzzjs8))
- Both the likelihood and the prior on their own are incomplete. They represent different sources of knowledge: the evidence you have here and now versus an understanding accumulated over time. When they agree, the outcome is easy. Otherwise, they exert their influence in proportion to their certainty. In the absence of clear prior knowledge, the likelihood dominates the decision. When the pull from the prior is strong, it can make you hardly believe your own eyes. In the presence of a strong prior, extraordinary claims can only be believed with extraordinary evidence ([View Highlight](https://read.readwise.io/read/01hq22rnazp9a24e9342njek6c))
- Bayes’ rule is a description of how to reason rationally under conditions of uncertainty. Therefore, humans should be using Bayes’ rule. Put simply, if evolution has done its job, we should see Bayes’ rule in the brain ([View Highlight](https://read.readwise.io/read/01hq22xs6tcz8vwthk9pza7bkr))
- This may seem simply like a list of our failings, but the researchers found that all of these lapses could be explained by a simple Bayesian model ([View Highlight](https://read.readwise.io/read/01hq23214dt4df1ty4g1j3e4fb))
    - Note: Subconscious optical illusions!
- Overall, perception could be the result of any complex combination of probabilities. The output of Bayes’ rule therefore provides a rich representation of sensory information, one that the brain can use in any way that seems most reasonable. In this way, probabilities mean possibilities ([View Highlight](https://read.readwise.io/read/01hq235gsqrztytgsdnadtnyvx))
    - Note: Same principle for the usefulness of Bayesian stats; can generate any distribution and also a point prediction by using a decision function.
- The Bayesian confidence hypothesis formalises this intuition by saying that how confident a person is in their interpretation of the world is directly related to the probability of that interpretation given the evidence – that is, the output of Bayes’ rule. ([View Highlight](https://read.readwise.io/read/01hq237r9kjkqx622wn5ve53kj))
- The free parameters of a model are all its movable parts – all the choices that the researcher can make when using it. The same way that, if given enough strokes even the worst golfer could eventually get the ball in the cup, if given enough free parameters any model can fit any data ([View Highlight](https://read.readwise.io/read/01hq23bkbwxza7mcsahfk6p5pk))
    - Note: Critique of Bayes, defining free parameters.
- Reinforcement learning is an explanation for how complex behaviour arises when simple rewards and punishments are the only learning signals. It is, in many ways, the art of learning what to do without being told. ([View Highlight](https://read.readwise.io/read/01hq23k3v6asvw6fkrgwjttpq6))
    - Note: Never knew about these origins of RL!
- Discounting is a way to weigh immediate gratification against delayed; it is ‘a bird in the hand is worth two in the bush’ codified into mathematics ([View Highlight](https://read.readwise.io/read/01hq23vpbjv9qdkcd17tz6kemr))
    - Note: Insights formalized by Bellman’s model
- To calculate the value at the next state, you simply assume that the best possible action is taken. And the best possible action is the one that leads to the state with the highest value ([View Highlight](https://read.readwise.io/read/01hq23y4d6zajdwk3m4xdrrk7h))
- This demonstrates how discounting future rewards can lead to plans with fewer steps; if there is only so much reward to get, it’s best to get it quicker rather than slower. Discounting also means that even big rewards will be ignored if they are too distant ([View Highlight](https://read.readwise.io/read/01hq24037w145wwbxjmhwa2x6j))
- ‘dynamic programming ([View Highlight](https://read.readwise.io/read/01hq241vadsb5xkpjhc9ymwh96))
    - Note: Cool conceptual formulation.
- What Sutton’s algorithm shows is that by mere exploration – simple trial and error – humans, animals or even an artificial intelligence can eventually learn the correct value function for the states they’re exploring. All it takes is updating expectations when expectations change – ‘learning a guess from a guess’ as Sutton describes ([View Highlight](https://read.readwise.io/read/01hq2489h99g7e5q2mwfkas02g))
- This is one of the roles of mathematics: to put questions disconnected in the physical world into the same conceptual space wherein their underlying similarities can shine through ([View Highlight](https://read.readwise.io/read/01hq24ax3vxd3fqhrr238brvpt))
    - Note: Convergent evolution in science via mathematical modeling!
- dopamine neurons encode the prediction errors necessary for temporal difference learning ([View Highlight](https://read.readwise.io/read/01hq24e147fydpa1d2dwv6pkyr))
- So, the neurons in the striatum don’t follow basic Hebbian learning. Instead they follow a modified form wherein the firing of one neuron before another only strengthens their connection if it happens in the presence of dopamine. Dopamine – which encodes the error signal needed for updating values – is thus also required for the physical changes needed for updating that occur at the synapse ([View Highlight](https://read.readwise.io/read/01hq24hbw4kb73qs3t5hyyktpm))
- An explanation encompassing all of Marr’s levels is an aspiration towards which many neuroscientists strive. The systems that carry out reinforcement learning are a rare case where they can come within striking distance of this high bar ([View Highlight](https://read.readwise.io/read/01hq24jymbpm76qf77mm42x9nd))
- It has long been thought that a theorist is considered great because his theories are true, but this is false. A theorist is considered great, not because his theories are true, but because they are interesting … In fact, the truth of a theory has very little to do with its impact, for a theory can continue to be found interesting even though its truth is disputed – even refuted ([View Highlight](https://read.readwise.io/read/01hq24pq9m88jwfqqfvbb07h07))
- free energy is the difference between the brain’s predictions about the world and the actual information it receives. The free energy principle says that everything the brain does can be understood as an attempt to minimise free energy – that is, to make the brain’s predictions align as much as possible with reality ([View Highlight](https://read.readwise.io/read/01hq24qksa1caknpheby2zevxq))
- Making the brain a better predictive machine might seem like the most obvious way of minimising free energy, but it’s not the only one. Because free energy is the difference between the brain’s prediction and experience, it can also be minimised by controlling experience. ([View Highlight](https://read.readwise.io/read/01hq284955n2xah1s1dx5ye8kw))
- As seen throughout this book, mathematical models of the brain are usually built by first identifying – out of the mounds of available data – a selection of facts that seem relevant. Those facts are then simplified and pieced together in a way that demonstrates how, in theory, a bit of the brain could work. Additionally, in figuring out how exactly to cobble this toy version of biology together, some new and surprising predictions may be made. ([View Highlight](https://read.readwise.io/read/01hq28aaj6npt136pw7bk5g3ym))
