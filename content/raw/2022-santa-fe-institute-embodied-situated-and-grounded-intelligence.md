---
ref-title: "Embodied, Situated, and Grounded Intelligence: Implications for AI"
ref-author: Santa Fe Institute
ref-publish-date: "2022"
ref-type: Workshop
ref-read-date: 2023-12-24
ref-link: https://arxiv.org/abs/2210.13589
aliases: 
tags:
  - raw
date: 2023-12-24
lastmod: 2023-12-28T18:08:18-08:00
---
# Highlights
- Her argument is based on her commitment to “semantic efficacy,” the view that the meaning (or content) of mental states makes a causal difference to what agents do and how they affect their environments. ([View Highlight](https://read.readwise.io/read/01hjf222ts56ym21fffgacqsjq))
- semantic externalism—the view that the meaning or “content” of a mental state depends on how one is situated in one’s environment. Together, these positions suggest that to understand how cognition shapes the behavior of agents, one must think about how agents are situated in their environments. ([View Highlight](https://read.readwise.io/read/01hjf22t7jcaf6bpdqcqcz6b06))
- The former describes the operation of the mechanism at a physical, mechanical, or (more commonly) algorithmic level. The latter expresses the significance of elements of that description in light of how the mechanism figures in an agent’s cognitive economy and how the agent is embedded in its environment. ([View Highlight](https://read.readwise.io/read/01hjf252dsgdsmnmgbjwn3b6kp))
- Hence, it seems like we could give a narrow, content-neutral description of how each circuit operates (e.g., identifying the algorithm it uses to processes inputs and produce outputs), then ascribe a meaning to its output based on wider considerations. ([View Highlight](https://read.readwise.io/read/01hjf2561zn6zd1m4yj1vmrs8z))
- Miracchi contends that this approach amounts to a rejection of semantic efficacy,
  since it suggests that the physical, mechanical, or algorithmic features of a cognitive system determine how it operates, and it treats the meaning of the output signal as a mere interpretive gloss, contextualizing but not influencing the operation of the circuits. ([View Highlight](https://read.readwise.io/read/01hjf25xnm063dreb2d6yzmbts))
- Mental content is better understood as a consequence of coordinating a diversity of internal mechanisms and external processes. ([View Highlight](https://read.readwise.io/read/01hjf26949az5a41vgxgvczsa6))
- we cannot hope to characterize all cognitive processes in the abstract (i.e., without considering those elements of the world to which they are related) since cognitive processes involving brainenvironment coupling depend on causal interactions between internal mechanisms and the wider environment. ([View Highlight](https://read.readwise.io/read/01hjf27k6h6fyj4csw4pwxp77t))
- This makes the referents of our mental representations an active part of ongoing cognition and not merely part of an interpretative gloss of an abstractly characterized cognitive mechanism. ([View Highlight](https://read.readwise.io/read/01hjf27zxtr4txbqha6971d8kw))
- This approach does not deny the importance of mental content ascriptions in interpreting and predicting the behavior of cognitive systems, but it emphasizes that the design challenge of developing such systems (i.e., the challenge we face in AI) is precisely the challenge of getting meaningless mechanisms to exhibit this kind of sophistication ([View Highlight](https://read.readwise.io/read/01hjf28y41awedt4654sct4xze))
- The cognitive revolution refers to the rise of modern cognitive science and the decline of behaviorism as the dominant approach in psychology ([View Highlight](https://read.readwise.io/read/01hjf2a0fthzg6pybebjzc5s2j))
- cognition should be understood as computation over representations bearing non-derived content, that is, content that does not depend on the attributions, interpretations, conventions, etc. of other agents. ([View Highlight](https://read.readwise.io/read/01hjf2a7zcfwsqvh84cs8g79rs))
- this distinction between cognition and behavior is central to the cognitive
  revolution and that neglecting it undersells the importance of our internal cognitive processes ([View Highlight](https://read.readwise.io/read/01hjf2az0931y7vq4mg32176za))
- radical embodiment theorists often fail to adequately distinguish between cognition itself and the causal contributors to cognition and cognitive development. ([View Highlight](https://read.readwise.io/read/01hjf2c8tr67v7r7t3he7ydzhj))
- “Meaning is a dynamic pattern of information that is made active transiently, as needed, in response to internally-generated or external cues.” ([View Highlight](https://read.readwise.io/read/01hjf2dcpkf0tkr64c2we9nfey))
- experimental interventions (e.g., using transcranial magnetic stimulation) had shown that impairment of motor regions impairs understanding of relevant words ([View Highlight](https://read.readwise.io/read/01hjf2facjkgna3b1f1a9x21sw))
- Smith suggested that some systems (such as cognitive systems) are only intelligible in terms of things that don’t make an immediate causal difference in those systems ([View Highlight](https://read.readwise.io/read/01hjf2mnrrbxm8fbbk2h1dse0p))
    - Note: Seems related to the notion of complexity as a whole; could we say that the “simple components” don’t explain the emergent behavior? Then what else?
- The task of explaining how the circuit works requires to say what it is about the system that makes the binary adder interpretation unique in its ability to render the behavior of the circuit intelligible. This suggests that the intelligibility of cognitive processes may not depend whether we actually regard the meaning of a mental representation as making a causal difference. ([View Highlight](https://read.readwise.io/read/01hjf2sahfqecvwpsz7m3b9wfz))
    - Note: Call back to “Could a neuroscientist understand a microprocessor” paper… need to revisit!
- our experience of learning language and its formal structure provides us with a valuable set of cognitive tools that we can deploy across a range of different tasks. This view he calls “LENS Theory” or “Language is an Embodied Neuroenhancement and Scaffold.” ([View Highlight](https://read.readwise.io/read/01hjf2vqd15a7qt4w6sneeafjp))
- The
  symbol grounding problem is the problem of how internal formal symbols (e.g., those realized in a mind or AI program) come to mean or refer to things in the world. ([View Highlight](https://read.readwise.io/read/01hjf2w2d2nsssby1a7ze7fcsq))
- If the meanings of symbols (including words) are firmly grounded in our sensory-motor systems, what explains our facility with uses of symbols that run roughshod over their normal sensory-motor associations (e.g., reasoning about the actions of an international “body” of scientists or the social consequences of a “family” of emerging technologies)? This is the symbol ungrounding problem. ([View Highlight](https://read.readwise.io/read/01hjf2whnkyyqx9thy05e276cy))
- language enhances our capacity for abstract thought but is not strictly necessary for it ([View Highlight](https://read.readwise.io/read/01hjf2yvvckxhsf31pgr1x2k43))
- major goal of this research is to determine which features of words (e.g., co-occurrence statistics) best predict human similarity judgments. ([View Highlight](https://read.readwise.io/read/01hjf32kmw3re34j1x31fp40bs))
- it suggests that words with similar sensorimotor associations will be judged to be more similar in general ([View Highlight](https://read.readwise.io/read/01hjf326pc1wb2t8pdmmv4qw4z))
- While models that used sensorimotor associations as predictors were consistently successful, these models were never the best predictor for human performance on any of the word similarity tasks. The best predictor was always one of the two distributional models, but one of the two distributional models was also among the worst predictors for each data set ([View Highlight](https://read.readwise.io/read/01hjf339bmswnez3zkv5wgttex))
- Comparing distributional and sensorimotor models, Connell found that the former were more likely to predict the first word produced, but the latter were more likely to predict where in a sequence a word was mostly likely to appear ([View Highlight](https://read.readwise.io/read/01hjf33x87rrape9hz6a0nzc9b))
- This suggests, she argues, that distributional information might be employed as a convenient
  heuristic for producing examples, but that when generating less closely associated examples, we engage sensorimotor concept representations ([View Highlight](https://read.readwise.io/read/01hjf3453kyab1n0415ffrvc58))
- follow-up research could look at sensorimotor dimensions with finer granularity (e.g., distinguishing different kinds of haptic feedback) or try to identify distinct similarity measures used by humans for different clusters of words. ([View Highlight](https://read.readwise.io/read/01hjf354h87phvm1m1rn19ap2d))
- further work might uncover more fundamental ways in which the brain represents categories. For example, some suggested that sensorimotor representations might play this role, while others favored causal network representations ([View Highlight](https://read.readwise.io/read/01hjf35khy4y5hvkm0qajw21ew))
- predict that words with greater sensorimotor involvement should be easier to recognize, process, and remember. ([View Highlight](https://read.readwise.io/read/01hjf368y94hq7cv361eajz3d6))
- Pexman quantified sensorimotor involvement as “Body-Object Interaction” (hereafter, “BOI”), asking study participants to rate “how easily a human body can interact with a word’s referent.” ([View Highlight](https://read.readwise.io/read/01hjf36madtfdqhkvkyjns0zqe))
- Despite the prevalence of factors weighting sensorimotor dimensions, the network analysis revealed that a word’s association (or lack thereof) with interoception (i.e., the ability to discern one’s internal states such such hunger or fatigue) was the most central of the dimensions ([View Highlight](https://read.readwise.io/read/01hjf38eby5wk35yj9qp0z49rb))
- Pexman argued that her results support a multi-modal view of how concepts are grounded, where the most important dimensions in each case are shaped by the task demands. ([View Highlight](https://read.readwise.io/read/01hjf38qw4v8amkd678zs3nxp7))
- Pexman clarified that she does not believe her work supports strong embodiment positions but does support multi-modal views that take seriously the sensorimotor contribution to grounding. ([View Highlight](https://read.readwise.io/read/01hjf39kb7syxj6351d8n2fz8n))
- The ensuing discussion suggested deeper disagreements about exactly what would constitute evidence for simulations in the brain and, more generally, for the idea that sensory motor areas play a causal role in reasoning about concepts. ([View Highlight](https://read.readwise.io/read/01hjf3akn45r0207wb0a8hve31))
- language can act as a kind of scaffolding in reinforcement learning, and that adding linguistic data to training improves performance and results in better abstraction–even in tasks where language is not directly relevant. He also suggested that concreteness might be better understood in relative terms. ([View Highlight](https://read.readwise.io/read/01hjf3b1gper41sncd4k3a0wwg))
- embodying existing systems might improve their performance and, especially, their robustness and generalization abilities ([View Highlight](https://read.readwise.io/read/01hjf3d2d3dtwmg514hxd3s215))
    - Note: Is reinforcement learning a form of embodiment? Can we make a model to simulate the benefits of embodiment for human cognition? Does this oppose the idea of emergent intelligence, to explicitly design this way— is that even a goal at all?
- embodiment or situatedness would be of much value to AI systems without substantial architectural changes that enhance abstract reasoning and allow these systems to leverage their experience in the world. ([View Highlight](https://read.readwise.io/read/01hjf3gh1zq61tt97zg533s4ce))
- learn from those experiences, and produce behaviors that shape their experiences in the future. This feedback loop allows animals to produce behaviors that enhance their opportunities for learning ([View Highlight](https://read.readwise.io/read/01hjf3hgacpwq0cnyjqcze1js8))
- Smith argued for three key theses in her talk: (1) Bodily development determines what data infants have access to, (2) behavior creates data for learning in real-time, and (3) the systems of knowledge are shaped by the constraints of space, time, and context ([View Highlight](https://read.readwise.io/read/01hjf3hts8shf9s6nvd0b6hgqq))
- objects that were close by and centered in their field of view when named were learned more reliably. In addition, other factors such head stability, head-eye alignment, and whether the child held the object also contributed to success. ([View Highlight](https://read.readwise.io/read/01hjf3jxksr3zd42qsemnwgj12))
- This tendency to orient objects in informative ways is not entirely surprising given the kinds
  of physical constraints people have (e.g., our bodies have a preferred up-down orientation due to gravity as well as numerous symmetries) ([View Highlight](https://read.readwise.io/read/01hjf3kdxkqpt4709h5qnfsvpe))
    - Note: Interesting to ask how much of embodiment is compensation for constraints! Maybe developing intelligence in a human manner is not the way…
- ability to learn from this kind of data is not the same as a domain-general learning system that leverages its own behavior to enhance learning across contexts (i.e., by generating useful data particular to that context) ([View Highlight](https://read.readwise.io/read/01hjf3ncfhjrr1q1dgenr9fxft))
- derstand how one might interact with it. ([View Highlight](https://read.readwise.io/read/01hjf3pngza38kxbn5bg515wdj))
- latent semantic analysis models struggled to distinguish objects whose affordances were appropriate in a given context from objects whose affordances were not appropriate ([View Highlight](https://read.readwise.io/read/01hjf3qd0zpsx44g565axjefkq))
- Glenberg hypothesizes that humans understand many affordances through sensorimotor simulations of the relevant contexts and objects and that language models struggle because they rely on distributional information alone. ([View Highlight](https://read.readwise.io/read/01hjf3qj507c9axefy8mjj23v5))
- Linda Smith suggested that future work might go beyond the kinds of training data ordinarily used for training these models (large corpora of written texts) and use the kind of language parents use with children. This, she argued, might give us a better sense of what can be learned from language—which she noted was especially helpful for children with disabilities (e.g., blindness) and could substantially compensate for their deficits. ([View Highlight](https://read.readwise.io/read/01hjf3rrrwmkc9nh06vdrah370))
- for solving problems. ([View Highlight](https://read.readwise.io/read/01hjf45sf760tjn38m8eahthg0))
- What these systems mainly lack are mechanisms for engaging in the slower, sequential, and rule-based reasoning that characterizes “System 2” (Kahneman, 2011; Evans & Stanovich, 2013). ([View Highlight](https://read.readwise.io/read/01hjf45zcs4jx0s8j8na11qtfw))
- System 2, Bengio suggests, is primarily responsible for the ability of humans to generalize their knowledge to a wide range of domains and to acquire new knowledge without the extensive training required by deep neural networks. ([View Highlight](https://read.readwise.io/read/01hjf466c6affq4z9f9m6b0r5q))
- Bengio, following Dehaene (2014) and others, holds to a global workspace view of higherlevel reasoning in humans. On this model, a number of more specialized modules compete for limited opportunities to broadcast information in a shared workspace. ([View Highlight](https://read.readwise.io/read/01hjf46f2t0v7j8djrjfv52ewm))
- The result is that cognition combines thoughts into a task-appropriate sequence for processing incoming information. ([View Highlight](https://read.readwise.io/read/01hjf46xtnr32hs2nq0kv7mw5m))
- More specifically, at each time step, different parts of the network compete to produce an activation state that becomes parts of the network’s input at the next time step. We can interpret this network as learning the transition probabilities between states and model this information as a graph of states with edges representing transition probabilities. ([View Highlight](https://read.readwise.io/read/01hjf47her5dasa2be2x790y0k))
- Early results with this architecture are promising, but for present purposes, the key results are (i) the identification of a gap in existing deep learning architectures (i.e., the absence of System 2 reasoning) and (ii) an implementable hypothesis about how this gap might be filled. ([View Highlight](https://read.readwise.io/read/01hjf47yhzj5sj0axa4j9kh2k7))
- is learning on evolutionary time scales which prepared brains to learn from their environments. There are also developmental timescales, in which learning occurs as the body and mind develop, and, finally, there are short timescales where we acquire particular knowledge or skills. ([View Highlight](https://read.readwise.io/read/01hjf4a26d4059vhrb10q3801z))
- On Cheney’s view, “embodiment” concerns the physical properties of an agent’s body, and “situatedness” concerns the role of an agent’s ongoing interactions with its environment ([View Highlight](https://read.readwise.io/read/01hjf4dbp6fvnr5tf8fjqf4z11))
- All this suggests that learning is improved when it takes place over multiple timescales, with the body adapting slowly and a control system adapting quickly. ([View Highlight](https://read.readwise.io/read/01hjf4ftkf0v25j4ry4dj7h4zq))
- Another commenter then added, responding to Mitchell’s first remarks, that cognitive science needs both diachronic (over time) explanation and synchronic (at a single time) explanation for our cognitive abilities. This was meant to suggest that while there may be very interesting evolutionary or developmental explanations of evolved designs, there remains an important question about how to explain the behavior of a system here and now. This continued a larger theme of the workshop—determining exactly how developmental history should figure in our explanations of cognition. ([View Highlight](https://read.readwise.io/read/01hjf4hv3yztbddy86zqcc2qq6))
- Foundation
  models (e.g., large language models) are deep neural networks pre-trained on large, broad data sets and then adapted to specific tasks. ([View Highlight](https://read.readwise.io/read/01hjf4j9910rks6wjkc5na2bgz))
- The foundation model paradigm might be distinguished from the transfer learning paradigm in that foundation models are not trained on one specific task and then asked to complete another. ([View Highlight](https://read.readwise.io/read/01hjf4jh9rpvd973ayq75nw257))
- Liang notes that, unlike foundation models, humans learn from multi-modal data, that our data involve actions and their consequences, that our own activity can generate new data, and that humans have a more structured and more modular neural architecture. ([View Highlight](https://read.readwise.io/read/01hjf4mpmbb0yk0ev5j749k3ht))
- To watch the entire talk, please visit: https://youtu.be/oD9d MZV5Og ([View Highlight](https://read.readwise.io/read/01hjf4r8hxvb5yjgnncn2shc8r))
- The first and more traditional method focuses on performance benchmarks designed to test specific competences. The other is more explicitly cognitive in nature and involves probing AI systems for causal models of relevant phenomena. ([View Highlight](https://read.readwise.io/read/01hjf4s57163m9pvxcdzt202mz))
- To watch the entire talk, please visit: https://youtu.be/BJ-8t8xrxq8 ([View Highlight](https://read.readwise.io/read/01hjf4r13xqprtz7e8kmphzf1e))
- First, there was general agreement that embodiment and situatedness play a large role in human development, but participants disagreed about what this implies about learning in AI systems. Second, there was some controversy about the degree to which embodiment and situatedness play an active role in ongoing cognitive processes (especially with respect to sensorimotor grounding and extended cognition). ([View Highlight](https://read.readwise.io/read/01hjf4v4cree248nxb8y6s6fya))
- Third, there was considerable controversy about whether distal or historical factors arising from the interplay of body and environment play an important explanatory (or perhaps constitutive role) in ongoing cognition. ([View Highlight](https://read.readwise.io/read/01hjf4v9zx64nwshcnvvj7ycgs))
- The most important (and least controversial) idea from the workshop was that embodiment and
  situatedness play an important role in human development. ([View Highlight](https://read.readwise.io/read/01hjf4w0tja4cq9n8n084e3490))
- Self-generated learning occurs when learners shape the data from which they learn. Smith’s talk, for example, emphasized the importance of physical interaction and exploration in learning and development. Children move around their environment, pick up, examine, and manipulate objects, track their parents’ gaze, etc. ([View Highlight](https://read.readwise.io/read/01hjf4wh0rskpckxjkxctpkb0h))
- of causation (e.g., Pearl, 2000; Woodward, 2003), some argued that intervening on one’s environment was necessary for testing and refining one’s hypotheses about the world’s causal structure ([View Highlight](https://read.readwise.io/read/01hjf4xkjdm8wqqzhrwv5b02w8))
- Understanding how the brain succeeds in connecting mental symbols to the external world is the symbol grounding problem. There are at least two ways one might frame this problem. ([View Highlight](https://read.readwise.io/read/01hjf4zp94vxhr6vb8v7kw03fg))
- Returning to the main issue, what might each of these accounts (i.e., sensorimotor grounding and mental content externalism) tell us about the role for embodiment/situatedness in ongoing cognition? ([View Highlight](https://read.readwise.io/read/01hjf51zmbpygeztd5043re6f8))
- That said, getting this kind of rich data is much more natural for embodied agents and there was little resistance to making learning for AI systems more like learning in embodied agents or to the idea that doing so might help AI systems to overcome some of their characteristic failure modes. Hence, while participants differed on what what necessary in principle, they more often agreed about what might useful for AI in practice. ([View Highlight](https://read.readwise.io/read/01hjf54t15jb9xsysrwmh4em3r))
- The idea here is that large language models seem to prioritize generating human-like text based on their sophisticated ability to learn distributional information about natural language from large corpora of text. This aim is not orthogonal to truth, but producing true statements seems at best a means to the end of producing human-like text. ([View Highlight](https://read.readwise.io/read/01hjf55amxv0cfjnf0e3rz4emf))
- The willingness of large language models to confidently confabulate, contradict themselves, or generate an arbitrary admixture of truth and falsehood suggest that they “communicate” in a very different way than their human interlocutors. ([View Highlight](https://read.readwise.io/read/01hjf55k251t8w6d5j85s407e9))
    - Note: Future directions of research: self contradiction in LLMs?
