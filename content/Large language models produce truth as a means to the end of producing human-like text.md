---
aliases: 
tags:
  - permanent-note
  - topic-cognitive-science
---
Large language models generate human-like text by learning distributions in natural language from “large corpora of text.” 

The tendency to prioritize truth-sounding statements instead of factual statements is considered a primary failure mode for today’s LLMs.

From [[2022-sfi-embodied-situated-and-grounded-intelligence]]:
>The willingness of large language models to confidently confabulate, contradict themselves, or generate an arbitrary admixture of truth and falsehood suggest that they “communicate” in a very different way than their human interlocutors.

#open-question *Can we really anthropomorphize LLMs in this way, as agents capable of communication and discourse?* AI is not [[Embodied cognition|embodied]] in the world it refers to; it has no goal of communicating factual information about the external world. 

