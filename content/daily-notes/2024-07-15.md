---
tags:
  - daily-note
lastmod: 2024-07-15T21:03:23-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?

- [ ] Make quiz for topic reliability, share with Simon and Marina
- [ ] Provide registration info for NSF REU
- [ ] Read paper on new methods from Simon

----
# Menu

> What other things can I do today that are less important?
## Today

- [ ] Train model on extended corpus
	- [ ] Finish scraping main blog posts – save into different paragraphs, with each paragraph as a new line for a text file
	- [ ] Make filtered list of “math words”

## Future file

- [ ] Watch more of course on renormalization
- [ ] Make literature note for *Being There*

---
# Media log

---
# Notes

#### Semantic word embeddings

- Corpus preprocessing
	- [ ] Filter English stop words (or top words?)
	- [x] Filter words appearing in >70 percent of documents
- Vocab preprocessing
	- [ ] Remove words shared across <5 percent of documents
	- [ ] (Potential) Use MALLET to separate into math topics more broadly, go through by hand?

#### Hyperbolic embeddings

```
import torch
import torch.nn as nn
import torch.nn.functional as F
import geoopt
from geoopt import ManifoldParameter
from geoopt.manifolds import PoincareBall

class ETM(nn.Module):
    def __init__(self, num_topics, embedding_matrix, t_hidden_size=100, curv=1.0):
        """
        Neural topic model using variational autoencoder with hyperbolic space.

        Args:
            num_topics (int) : number of topics
            embedding_matrix (numpy.array) : VxD matrix, where V is the vocabulary
                size and D is the embedding dimension
            t_hidden_size (int) : dimension of hidden space q(theta)
            curv (float) : curvature of the hyperbolic space
        """
        super(ETM, self).__init__()

        # Model parameters
        self.num_topics = num_topics
        self.vocab_size = embedding_matrix.shape[0]
        self.rho_size = embedding_matrix.shape[1]
        self.t_hidden_size = t_hidden_size
        self.curv = curv
        
        # Optimization hyperparameters
        self.enc_drop = 0.5
        self.t_dropout = nn.Dropout(self.enc_drop)
        self.theta_act = nn.ReLU()

        # Define word embedding on Poincare ball
        self.manifold = PoincareBall(c=self.curv)
        self.rho = ManifoldParameter(torch.from_numpy(embedding_matrix).float(), manifold=self.manifold)
        self.rho.requires_grad = False # freeze weights

        # Define topic embedding on Poincare ball
        self.alphas = ManifoldParameter(torch.empty(self.num_topics, self.rho_size), manifold=self.manifold)
        nn.init.uniform_(self.alphas, -1.0, 1.0)  # Initialize topic embeddings

        # Variational distribution for untransformed topic proportion
        self.q_theta = nn.Sequential(
            nn.Linear(self.vocab_size, self.t_hidden_size),
            self.theta_act,
            nn.Linear(self.t_hidden_size, self.t_hidden_size),
            self.theta_act
        )
        self.mu_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)
        self.log_sigma_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)

    def reparameterize(self, mu, log_sigma):
        """
        Returns a sample from the reparametrized Gaussian distribution in hyperbolic space

        Args:
            mu : mean
            log_sigma : variance
        """
        std = torch.exp(0.5 * log_sigma)
        eps = torch.randn_like(std)
        z = eps.mul_(std).add_(mu)
        return self.manifold.projx(z)

    def encode(self, bag_of_words):
        """
        Get parameters of variational distribution for \theta.

        Args:
            bag_of_words (torch.Tensor) : tensor of shape [batch_size] x [vocab_size]
        """
        q_theta = self.q_theta(bag_of_words)
        if self.enc_drop > 0:
            q_theta = self.t_dropout(q_theta)
        mu_theta = self.mu_q_theta(q_theta)
        log_sigma_theta = self.log_sigma_q_theta(q_theta)
        kl_theta = -0.5 * torch.sum(1 + log_sigma_theta - mu_theta.pow(2) - log_sigma_theta.exp())
        return mu_theta, log_sigma_theta, kl_theta

    def get_beta(self):
        """
        Get 'traditional' topic (i.e., distribution over words) induced by word
        embeddings \rho and topic embedding \alpha_k in hyperbolic space.
        """
        # Compute distances in hyperbolic space and transform to logits
        logit = -self.manifold.dist(self.alphas.unsqueeze(1), self.rho.unsqueeze(0))
        return F.softmax(logit, dim=1)

    def get_theta(self, normalized_bag_of_words):
        """
        Get topic proportion for the normalized "bag of words" document.
        """
        mu_theta, log_sigma_theta, kl_theta = self.encode(normalized_bag_of_words)
        z = self.reparameterize(mu_theta, log_sigma_theta) # topic assignment
        theta = F.softmax(z, dim=1)
        return theta, kl_theta

    def decode(self, theta, beta):
        """
        Get log-probabilities of a topic given a document, assigning small
        values to avoid computing log of 0.

        Args:
            theta (torch.Tensor)
            beta (torch.Tensor)
        """
        results = torch.mm(theta, beta)
        almost_zeros = torch.full_like(results, 1e-6)
        results_without_zeros = results.add(almost_zeros)
        return torch.log(results_without_zeros)

    def forward(self, bag_of_words, normalized_bag_of_words, theta=None):
        # Get topic proportions
        if (theta is None):
            theta, kl_theta = self.get_theta(normalized_bag_of_words)
        else:
            kl_theta = None

        # Get topic as a distribution over words
        beta = self.get_beta()

        # Get prediction loss
        preds = self.decode(theta, beta)
        reconstruction_loss = -(preds * bag_of_words).sum(1)
        reconstruction_loss = reconstruction_loss.mean() # aggregate
        return reconstruction_loss, kl_theta

```

**HyperETM v1**

```
class HyperETM(ETM):
    def __init__(self, num_topics, embedding_matrix, t_hidden_size=100, curvature=1.0, clip_radius=1.0):
        super(HyperETM, self).__init__(num_topics, embedding_matrix, t_hidden_size)

        self.curvature = curvature #  nn.Parameter(torch.Tensor[-1.])
        self.manifold = PoincareBall(c=self.curvature)

        # Define word embeddings
        self.rho = ManifoldParameter(torch.from_numpy(embedding_matrix).float(), manifold=self.manifold)
        self.rho.requires_grad = False # freeze weights

        # Define topic embeddings
        self.alphas = ManifoldParameter(torch.empty(self.num_topics, self.rho_size), manifold=self.manifold)
        nn.init.uniform_(self.alphas, -1.0, 1.0)  # initialize topic embeddings

        self.clip_radius = clip_radius
    
    def feature_clip(self, x):
          x_norm = x.norm(p=2, dim=-1, keepdim=True)
          cond = x_norm > self.clip_r
          projected = x / x_norm * self.clip_r
          return torch.where(cond, projected, x)

    def reparameterize(self, mu, log_sigma):
        std = torch.exp(0.5 * log_sigma)
        eps = torch.randn_like(std)
        z = eps.mul_(std).add_(mu)
        return self.manifold.projx(z)

    def get_beta(self):
        logit = -self.manifold.dist(self.alphas.unsqueeze(1), self.rho.unsqueeze(0))
        return F.softmax(logit, dim=1)
```
**v1.5**
```
        self.curvature = curvature
        self.manifold = PoincareBall(c=self.curvature) # use nn.Parameter(torch.Tensor[-1.]) to train parameter
        self.clip_radius = clip_radius

    def feature_clip(self, x):
        x_norm = x.norm(p=2, dim=-1, keepdim=True)
        cond = x_norm > self.clip_radius
        projected = x / x_norm * self.clip_radius
        return torch.where(cond, projected, x)
    
    def get_phi(self):
        hyp_rho = self.manifold.projx(self.manifold.expmap(self.feature_clip(self.rho)))
        hyp_alpha = self.manifold.projx(self.manifold.expmap(self.feature_clip(self.alpha)))
        return torch.softmax(-self.manifold.dist(
            hyp_rho.unsqueeze(1), hyp_alpha.unsqueeze(0)
        ), dim=0)

    def forward(self, x):
        
```

#### Basic ETM model

```
class ETM(nn.Module):
    # TO DO: encoder dropout? default 0.5
    def __init__(self, num_topics, embedding_matrix, t_hidden_size=100, clip_radius):
        """
        Neural topic model using variational autoencoder.

        Args:
            num_topics (int) : number of topics
            embedding_matrix (numpy.array) : VxD matrix, where V is the vocabulary
                size and D is the embedding dimension
            t_hidden_size (int) : dimension of hidden space q(theta)
        """
        super(ETM, self).__init__()

        # Model parameters
        self.num_topics = num_topics
        self.vocab_size = embedding_matrix.shape[0]
        self.rho_size = embedding_matrix.shape[1]
        self.t_hidden_size = t_hidden_size
        # emsize?
        # training?

        # Optimization hyperparameters
        self.enc_drop = 0.5
        self.t_dropout = nn.Dropout(self.enc_drop)
        self.theta_act = nn.ReLU()

        # Define word embedding
        self.rho = nn.Embedding(self.vocab_size, self.rho_size)
        self.rho.weight.data.copy_(torch.from_numpy(embedding_matrix).float())
        self.rho.weight.requires_grad = False # freeze weights

        # Define topic embedding
        self.alphas = nn.Linear(self.rho_size, num_topics, bias=False)

        # Variational distribution for untransformed topic proportion
        self.q_theta = nn.Sequential(
            nn.Linear(self.vocab_size, self.t_hidden_size),
            self.theta_act,
            nn.Linear(self.t_hidden_size, self.t_hidden_size),
            self.theta_act
        )
        self.mu_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)
        self.log_sigma_q_theta = nn.Linear(self.t_hidden_size, num_topics, bias=True)

    def feature_clip(self, x):
        x_norm = x.norm(p=2, dim=-1, keepdim=True)
        cond = x_norm > self.clip_radius
        projected = x / x_norm * self.clip_radius
        return torch.where(cond, projected, x)

    def reparameterize(self, mu, log_sigma):
        """
        Returns a sample from the reparametrized Gaussian distribution TO DO

        Args:
            mu : mean
            log_sigma : variance
        """
        std = torch.exp(0.5 * log_sigma)
        eps = torch.randn_like(std)
        return eps.mul_(std).add_(mu)

    def encode(self, bag_of_words):
        """
        Get parameters of variational distribution for \theta.

        Args:
            bag_of_words (torch.Tensor) : tensor of shape [batch_size] x [vocab_size]
        """
        q_theta = self.q_theta(bag_of_words)
        if self.enc_drop > 0:
            q_theta = self.t_dropout(q_theta)
        mu_theta = self.mu_q_theta(q_theta)
        log_sigma_theta = self.log_sigma_q_theta(q_theta)
        kl_theta = -0.5 * torch.sum(1 + log_sigma_theta - mu_theta.pow(2) - log_sigma_theta.exp())
        return mu_theta, log_sigma_theta, kl_theta

    def get_beta(self):
        """
        Get 'traditional' topic (i.e., distribution over words) induced by word
        embeddings \rho and topic embedding \alpha_k.
        """
        logit = self.alphas(self.rho.weight) # torch.mm(self.rho.weight, self.alphas)
        return F.softmax(logit, dim=0).transpose(1, 0) # softmax over vocab dimension

    def get_theta(self, normalized_bag_of_words):
        """
        Get topic proportion for the normalized "bag of words" document.
        """
        mu_theta, log_sigma_theta, kl_theta = self.encode(normalized_bag_of_words)
        z = self.reparameterize(mu_theta, log_sigma_theta) # topic assignment
        theta = F.softmax(z, dim=1)
        return theta, kl_theta

    def decode(self, theta, beta):
        """
        Get log-probabilities of a topic given a document, assigning small
        values to avoid computing log of 0.

        Args:
            theta (torch.Tensor)
            beta (torch.Tensor)
        """
        results = torch.mm(theta, beta)
        almost_zeros = torch.full_like(results, 1e-6)
        results_without_zeros = results.add(almost_zeros)
        return torch.log(results_without_zeros)

    def forward(self, bag_of_words, normalized_bag_of_words, theta=None):
        # Get topic proportions
        if theta is None:
            theta, kl_theta = self.get_theta(normalized_bag_of_words)
        else:
            kl_theta = None

        theta = self.feature_clip(theta)

        # Get topic as a distribution over words
        beta = self.get_beta()

        # Get prediction loss
        preds = self.decode(theta, beta)
        reconstruction_loss = -(preds * bag_of_words).sum(1)
        reconstruction_loss = reconstruction_loss.mean() # aggregate
        return reconstruction_loss, kl_theta

class HyperETM(ETM):
    def __init__(self, num_topics, embedding_matrix, t_hidden_size=100, curvature=1.0, clip_radius=1.0):
        super(HyperETM, self).__init__(num_topics, embedding_matrix, t_hidden_size)
        self.curvature = curvature 
        self.manifold = PoincareBall(c=self.curvature)

        # Define word embeddings
        self.rho = ManifoldParameter(torch.from_numpy(embedding_matrix).float(), manifold=self.manifold)
        self.rho.requires_grad = False # freeze weights

        # Define topic embeddings
        self.alphas = ManifoldParameter(torch.empty(self.num_topics, self.rho_size), manifold=self.manifold)
        nn.init.uniform_(self.alphas, -1.0, 1.0)  # initialize topic embeddings

        self.clip_radius = clip_radius

    def reparameterize(self, mu, log_sigma):
        std = torch.exp(0.5 * log_sigma)
        eps = torch.randn_like(std)
        z = eps.mul_(std).add_(mu)
        return self.manifold.projx(z)

    def get_beta(self):
        logit = -self.manifold.dist(self.alphas.unsqueeze(1), self.rho.unsqueeze(0))
        return F.softmax(logit, dim=1)
```