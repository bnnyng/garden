---
tags:
  - daily-note
lastmod: 2024-07-24T17:52:00-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?



----
# Menu

> What other things can I do today that are less important?
## Today

**Morning**
- [ ] Remove final comment group from diversity computation when there is a “remainder”
- [x] Look at comments with a high word count
- [x] Use log-scale for scatterplot x-axis
## Future file

- [ ] Try document vector analysis, perform unsupervised clustering?

---
# Media log

---
# Notes

“Rough summary” of morning results:
```
1. There are some challenges in estimating topic compositions when the number of topics is large and the comments are short. But we can probably do up to 100 topics, if we cut at comments >20 words. If we don’t do word cuts, we get strong covariances, particularly at 100 or 200 topics.
2. Pretty consistently, and rather invariant to choice of subgroup size, number of topics, or whether we build the topic model using only P1 data, or using all data, we find that the diversity is low in the first 20% of the data, then jumps up, and stays relatively stable for the remainder of the data. Eyeballing the standard errors, that result is probably quite statistically significant.
```

- New parameters: subgroup size 20, 20 topics, no cutoffs, all data
- Noticed error in getting “all comment data”

#### Brain dump

- What would be the goal of extracting “cognitive moves”?
	- Showing people’s preferences
	- Looking at distribution over the course of projects (similar to diversity) – the area covered, correspondence with time, correspondence with diversity and exploration
		- How do people collaborate to explore or to exploit?

#### Cognitive words clustering

- Normalizing vectors – makes sense to normalize within each document, since we want each document to take equal importance?
- Clustering methods
	- DBSCAN – more robust to noise and outliers (will not cluster them), 

#### Examining scatterplots

- More total topics corresponds with stronger correlation between log of comment length and entropy – “it’s harder to estimate 100 topics than 10 topics, given a short text”
	- Correlations between comment length and entropy are **unwanted** because, e.g., if comment length is changing over time, then that can affect diversity measure
- Plotting interpretations
	- Covariance is low regardless of cutoff 10 topics
	- In general, higher number of topics leads to stronger covariance
	- Using minimum cutoff 20 allows for more topics

#### MALLET knob turning

- Currently “up in the air”
	- Comment length cut-offs

**Cognitive topics**
```
bin\mallet import-file --input data\comments-inclusion-mallet.tsv --output cognitive.mallet --keep-sequence 
```

```
bin\mallet train-topics --input cognitive.mallet --num-topics 5 --output-state cognitiveOutput5.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics cognitiveTopics5.txt --output-topic-keys cognitiveKeys5.txt 
```

```
bin\mallet train-topics --input cognitive.mallet --num-topics 10 --output-state cognitiveOutput10.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics cognitiveTopics10.txt --output-topic-keys cognitiveKeys10.txt 
```

**All comment data – semantic**
```
bin\mallet import-file --input data\mallet-semantic.tsv --output semantic-all.mallet --keep-sequence 
```

10 topics
```
bin\mallet train-topics --input semantic-all.mallet --num-topics 10 --output-state semanticOutput10-all.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics10-all.txt --output-topic-keys semanticKeys10-all.txt 
```


20 topics
```
bin\mallet train-topics --input semantic-all.mallet --num-topics 20 --output-state semanticOutput20-all.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics20-all.txt --output-topic-keys semanticKeys20-all.txt 
```

50 topics
- For all comments: -7.96962 
```
bin\mallet train-topics --input semantic-all.mallet --num-topics 50 --output-state semanticOutput50-all.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics50-all.txt --output-topic-keys semanticKeys50-all.txt 
```

100 topics
```
bin\mallet train-topics --input semantic-all.mallet --num-topics 100 --output-state semanticOutput100-all.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics100-all.txt --output-topic-keys semanticKeys100-all.txt 
```

200 topics
```
bin\mallet train-topics --input semantic-all.mallet --num-topics 200 --output-state semanticOutput200-all.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticTopics200-all.txt --output-topic-keys semanticKeys200-all.txt 
```