---
tags:
  - daily-note
lastmod: 2024-07-16T16:27:59-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?

- [x] Start “topics” quiz – complete first 10 before meeting with Simon and Marina

----
# Menu

> What other things can I do today that are less important?
## Today

- [ ] Top2Vec tests
	- [x] Make custom id for main posts
	- [x] Fit Top2Vec model on all posts and comments *with IDs*
- [ ] Top2Vec evaluations
	- [ ] Lower-dimensional visualization
	- [ ] Compute areas (over 10 percent of comments at a time?)

## Future file

- [ ] Embedded topic model
	- [ ] Get better corpus for word embeddings, e.g. Wikipedia
	- [ ] Implement evaluation metrics
	- [ ] Try smaller window, no bi-grams

---
# Media log

---
# Notes

#### Measuring diversity of comments

```
Let’s start with the case of a single probability distribution, p(t). This could be a distribution over topics, for example. We want to say, given a distribution p(t), how diverse is the underlying document. If p(t) is all focused on one topic, that’s low diversity. If it’s spread uniformly over all topics, that’s high diversity.
2:51
Shannon solved this problem in a very elegant way, and showed that there was one unique measure, “entropy”, usually written H(p), that told you how diverse the distribution was.
2:52
If p(t) is all on one topic, it’s 0. If p(t) is uniformly spread over N topics, it’s log_2(N)
2:53
You can use this trick to measure the diversity of topics within a comment. You can also use it to measure the diversity of commenters in a group of topics — define p(x) as the probability that if you draw a comment at random from the group, you get person x. Then the entropy of p(x) is the person diversity.
2:53
Now, there’s a second question… how do you measure the diversity of a group of comments! i.e., the diversity of a collection of probabilities p_1(x), p_2(x), etc.
2:55
One thing you might think of doing is just adding all the probabilities together, and taking the entropy. But that misses something: e.g., imagine a pair of comments with [0,1] and [1,0] as their probabilities over two topics. The average is [1/2,1/2], and the entropy is high. But you can also image a pair of comments with [1/2, 1/2] and [1/2, 1/2]. That would produce the same average, but the intuition is that the second pair is less diverse, because they’re both the same: equal mixtures of topic 1 and topic 2.
```

#### HMM on Top2Vec topics

```
def get_num_hidden_states(data, num_states=[2, 4, 6, 8, 10]):
    kf = KFold(n_splits=5)
    cross_val_scores = {}
    for n in num_states:
        scores = []
        for train_index, test_index in kf.split(data):
            X_train, X_test = data[train_index], data[test_index]
            model = hmm.GaussianHMM(n_components=n, covariance_type='diag', n_iter=1000)
            model.fit(X_train)
            log_likelihood = model.score(X_test)
            scores.append(log_likelihood)
        cross_val_scores[n] = np.mean(scores)
        print(f"Number of states: {n}, Cross-Validation Score: {np.mean(scores)}")
    # Select the model with the highest average score
    optimal_n = max(cross_val_scores, key=cross_val_scores.get)
    return optimal_n

def plot_hidden_states(df, data, hidden):
    indices = np.arange(0, len(df))
    plt.figure(figsize=(12, 6))

    # Plot observations
    plt.subplot(2, 1, 1)
    plt.plot(indices, data, marker='o', linestyle='-', color='blue')
    plt.title('Observed Topics')
    plt.xlabel('Comment Index')
    plt.ylabel('Topic Value')

    # Plot hidden states
    plt.subplot(2, 1, 2)
    plt.plot(indices, hidden, marker='o', linestyle='-', color='red')
    plt.title('Inferred Hidden States')
    plt.xlabel('Comment Index')
    plt.ylabel('State')

    plt.tight_layout()
    plt.show()
```

#### Computing diversity

```
def jensen_shannon_distance(p, q):
    # Convert inputs to numpy arrays and ensure they are valid probability distributions
    p = np.array(p, dtype=np.float64)
    q = np.array(q, dtype=np.float64)
    
    # Ensure distributions sum to 1 (for safety)
    p = p / np.sum(p)
    q = q / np.sum(q)
    
    # Compute the midpoint distribution M
    m = 0.5 * (p + q)
    
    # Compute the Kullback-Leibler divergence between p and m, and q and m
    kl_pm = kl_div(p, m).sum()
    kl_qm = kl_div(q, m).sum()
    
    # Jensen-Shannon Divergence
    js_divergence = 0.5 * (kl_pm + kl_qm)
    
    # Jensen-Shannon Distance (square root of divergence)
    js_distance = np.sqrt(js_divergence)
    
    return js_distance
```