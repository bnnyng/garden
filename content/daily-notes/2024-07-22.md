---
tags:
  - daily-note
lastmod: 2024-07-22T17:58:57-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?

- [ ] Prepare topics for computing diversity
	- [x] Create distributions for Top2Vec results
	- [ ] Filter number of topics by removing or renormalizing “junk” topics
- [x] Get diversity measures working with topics generated by model
	- [x] Check JSD bounds
- [x] Display plots more neatly

----
# Menu

> What other things can I do today that are less important?
## Today

- [x] Play with reduced topics – count number of topics that could be reasonably consolidated
- [ ] Review literature
	- [ ] Scott Page on diversity in group problem-solving
	- [ ] DeDeo’s original paper on exploration/exploitation
## Future file

---
# Media log

---
# Notes

#### Cleaning up topics

- Hierarchical topic reduction process
	- For big topics with >500 documents, look for other topics that have shared words. If more than 10 words are shared (in particular, given list of topics for each shared word, the overlapping topics appear more than 10 times – *why can this happen?*), add to count.
	- To get final number of reduced topics, remove number of topics in the overall “overshared set,” minus 100 to account for other simplification possibilities
- After hierarchical topic reduction, identify topics where heavily weighted comments should be removed entirely, and topics where heavily weighted comments should be renormalized such that the topic no longer exists
	- Iterate on this renormalization process for big topics until the distribution of comments across possible topics is improved
	- *What is the relationship between renormalizing and computing diversity? Remove rows from topic array, then re-compute distributions?*
		- How would this change diversity computations? 

#### Computing topic distribution

- Currently: making all values positive by adding the minimum value, then renormalizing to be between 0 and 1
- Other possibilities – note that negative values mean opposite relationships, and 0 values mean no relationship; what does it make sense to do with negative values in a semantic context?
	- [x] Check magnitudes of both positive and negative values
	- Clipping to 0 does not make much sense!

#### Diversity computations

- Possible parameters to change
	- Different group sizes; groups as a fraction of overall comments
	- Using document vectors directly or top topic vectors
	- Using reduced topic vectors

#### Summary of overall procedure

```
1. I preprocessed comments by removing all documents with "meta, metacomment" etc and documents with fewer than 3 words. My corpus was all blog posts and comments, plus Wikipedia pages for math topics.
2. For the "original topics," I trained an embedded topic model (top2vec), which automatically returned 681 topics.
3. I estimated an appropriate number of "reduced topics" by examining original topics which had over 500 documents, and counting the number of other topics which shared top words (this feels like a similar sort of "art" as deciding on the number of topics for the usual LDA model). The idea was that if over 10 top words were shared, I would want to combine the topics. The reduction algorithm then merges most similar topics (based on cosine similarity between topic vectors) until the desired number of topics is reached. This gave 395 reduced topics.
4. To compute the distribution for a given comment, I computed the cosine similarity between the comment vector and each topic vector and normalized the similarities as a distribution that sums to 1.
5. For "filtered topics," I inspected the reduced topics for those that were dominated by non-mathy words, and looked at the comments for each non-mathy topic. If the comments generally had some mathy content, I removed the topic from each distribution and renormalized. If the comments had no math content (like "oops, fixed that"), I just deleted each the comments dominated by that topic. This left 386 topics.
6. In the diversity plots, the "Original topics" line was computed using the distribution with 681 topics, "Reduced topics" with the 395 topic distribution, and "Filtered topics" with the 386 topic distribution. The `diversity-filtered` plot is a bit misleading because the "Original topics" line also had comments deleted due to the way I set up my code, though the distributions themselves are unchanged.

I'd love to know general thoughts on whether we like this approach; I imagine topics can be cleaned/better refined as we examine critical points. (Also have to leave SFI now but will send more info later if needed)
```

#### Next steps
