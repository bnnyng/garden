---
tags:
  - daily-note
lastmod: 2024-08-02T09:31:38-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?

- [x] Compute hierarchies using word-topic matrix
- [ ] Prepare talk slides

----
# Menu

> What other things can I do today that are less important?
## Today

- Morning
	- [x] Prepare for meeting with Bob Axelrod – read papers, brainstorm questions
	- [x] Respond to Polymath researcher email
- Evening
	- [ ] Literature review for talk slides, aggregate notes
	- [x] Read about DeepMind’s [augmented AlphaProof](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/?utm_source=x&utm_medium=social&utm_campaign=&utm_content=)
- Other life admin
	- [x] Respond to email about CJUM launch event
## Future file

- [ ] (Temp) Look at reading queue, save papers for next week’s visitors
- [ ] Reflect on meeting with Bob Axelrod
- [ ] Look into making visualizations with Tableau, d3js
- [x] Make plans for course registration

---
# Notes

#### Computing hierarchies with word-topic matrix

From [[2024-07-31]]:
- Distribution for each word can be computed by `p(word|topic) = (count[topic, word] + alpha / num_word_types) / (sum(count[topic, w] for w in words) + alpha)`
- Resources
	- https://stackoverflow.com/questions/19661094/how-to-get-word-topic-probability-using-mallet
	- https://stackoverflow.com/questions/33251703/how-to-get-a-probability-distribution-for-a-topic-in-mallet#comment69702638_33251703

```
For 1, you represented each topic t as a list v(t) of d numbers, where d is the number of documents, and v(t)_i is the probability of topic t in document i. Notice that that’s not a probability distribution! (It doesn’t sum to one).
However, it looks like you then normalize it. This is sort of like saying “say I picked a word associated with topic t; what’s the probability that word came from document i?”

A very different way to do it is to look at the probability distribution over words. Remember, each topic is defined as a probability distribution (the “top words” in a topic are just the words with high probability in that topic).

So if you can figure out how to get access to those word distributions, you have a different way to compute distance: not “do these two topics tend to occur in the same documents”, but “do these two topics look similar in which words they use.”
```

#### Dendrogram attempt

```
from scipy.cluster.hierarchy import dendrogram

  

def plot_diversity_dendrogram_horizontal(title, linkage_matrix, df, dist_col, num_topics, num_groups=10):

    num_comments = len(df)

    subgroup_size = 0 if num_comments < 200 else 20

    div, ids, se = compute_diversity(df, dist_col=dist_col, num_groups=num_groups, subgroup_size=subgroup_size)

  

    overall_avg = []

    topic_labels = [i for i in range(num_topics)]

    for i in tqdm(range(num_groups), desc='Plotting dendrograms'):

        group_ids = ids[i]

        try:

            group_dists = np.array(df[dist_col].iloc[group_ids].tolist())

        except IndexError:

            print(i, group_ids)

        group_avgs = np.mean(group_dists, axis=0).tolist() # compute the mean of each topic column

        overall_avg.append(group_avgs)

        fig, ax = plt.subplots(figsize=(4, 4))

        dendro = dendrogram(linkage_matrix, orientation='left', no_plot=True)

        leaves = dendro['leaves']

        leaf_positions = {leaf: pos for pos, leaf in enumerate(leaves)}

        dendrogram(linkage_matrix, orientation='left', ax=ax)

  

        leaf_value_dict = {leaf : group_avgs[leaf] for leaf in leaves}

        height = 0.8  # Adjust the bar height for better visibility

        for j, (leaf, group_avg) in enumerate(leaf_value_dict.items()):

            leaf_y = leaf_positions[leaf]

            ax.barh(leaf_y, group_avg, height=height, align='center', color='blue', label=f'Topic {leaf}' if j == 0 else '')

        # Add legend outside the loop to ensure only one entry per topic

        handles, labels = ax.get_legend_handles_labels()

        by_label = dict(zip(labels, handles))

        ax.legend(by_label.values(), by_label.keys())

  

        plt.title(f'{title} - Group {i + 1}')

        plt.tight_layout()

        plt.show()

        return

  

# # Add bar charts

# for i, leaf in enumerate(leaves):

#     value1 = df_bar.loc[leaf, 'value1']

#     value2 = df_bar.loc[leaf, 'value2']

#     ax.barh(i, value1, height=0.4, align='center', color='blue', label='Value 1' if i == 0 else "")

#     ax.barh(i, value2, height=0.4, align='edge', color='green', label='Value 2' if i == 0 else "")

  

# # Add legend

# ax.legend()

  

# # Adjust layout

# plt.tight_layout()

# plt.show()

  

plot_diversity_dendrogram_horizontal(

    title=f'P1 Gowers Only - Average Topic Distribution Per Group - {NUM_REDUCED} Topic Distribution',

    linkage_matrix=linkage_matrix,

    df=df_comments,

    dist_col=f'distribution-{NUM_TOPICS}',

    num_topics=NUM_TOPICS

)
```