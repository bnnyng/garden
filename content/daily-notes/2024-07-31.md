---
tags:
  - daily-note
lastmod: 2024-08-01T18:09:38-06:00
---
>[!example] Reminder
>Be the most excellent version of myself I can be, one day at a time. If I look back on today specifically, I want to be proud of what I did and who I am.

# To do

> What are the top three *most important* things I need to do today?

- [x] Finish semantic word list for Polymath1
- [ ] Literature review and collect themes for final talk

----
# Menu

> What other things can I do today that are less important?
## Today

- Morning – wrapping up analyses
	- [x] Summarize [[2024-07-30]], plan the day
	- [x] Submit final talk title
	- [x] Plot v3 topic results and check entropy
	- [x] Send plot results to Slack
- Afternoon – final talk preparations
	- [x] Look into coarse-graining axioms of information theory
	- [ ] Compute difference between fine and coarse-grained diversities
	- [ ] Create plots with unwrapped dendrograms
- Evening
	- [ ] Get hierarchies using words for distance metric
	- [ ] Prepare for meeting with Bob Axelrod
## Future file

- [x] Organize files and make Polymath data public
- [ ] Semantic word list for Polymath4
- [ ] Look into next week’s visitor(s)
- [ ] Try different forms of hierarchical reduction
	- [ ] Measurements – cosine similarity, Euclidean distance
	- [ ] Hyperbolic geometry
- [x] Preprocessing adjustments
	- [x] Turn hyphens into spaces
	- [x] Lemmatize content
- [ ] Plot document-topic vectors
- [ ] Finish reading DeDeo’s article on information theory
- [ ] Update ETM repo with hyperbolic code

---
# Notes

#### Hierarchy with word probabilities

- Distribution for each word can be computed by `p(word|topic) = (count[topic, word] + alpha / num_word_types) / (sum(count[topic, w] for w in words) + alpha)`
- Resources
	- https://stackoverflow.com/questions/19661094/how-to-get-word-topic-probability-using-mallet
	- https://stackoverflow.com/questions/33251703/how-to-get-a-probability-distribution-for-a-topic-in-mallet#comment69702638_33251703

```
For 1, you represented each topic t as a list v(t) of d numbers, where d is the number of documents, and v(t)_i is the probability of topic t in document i. Notice that that’s not a probability distribution! (It doesn’t sum to one).
However, it looks like you then normalize it. This is sort of like saying “say I picked a word associated with topic t; what’s the probability that word came from document i?”

A very different way to do it is to look at the probability distribution over words. Remember, each topic is defined as a probability distribution (the “top words” in a topic are just the words with high probability in that topic).

So if you can figure out how to get access to those word distributions, you have a different way to compute distance: not “do these two topics tend to occur in the same documents”, but “do these two topics look similar in which words they use.”
```
#### Semantic and semantic “inclusion” lists – Polymath1, Gowers’ blog only

- V3 performance – Polymath1, Gowers’ blog only
	- 10 topics: LL -4.36132
	- 15 topics: LL -4.34106
	- 20 topics: LL -4.27747
	- 30 topics: LL -4.26412
	- 40 topics: LL -4.24676

Import data
```
bin\mallet import-file --input data\semantic-seeded-P1-onlyG.tsv --output semantic-seeded-P1-onlyG.mallet --keep-sequence 
```

10 topics
```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 10 --output-state semanticSeededOutput10-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics10-P1-onlyG.txt --output-topic-keys semanticSeededKeys10-P1-onlyG.txt 
```

15 topics
```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 15 --output-state semanticSeededOutput15-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics15-P1-onlyG.txt --output-topic-keys semanticSeededKeys15-P1-onlyG.txt 
```


20 topics
```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 20 --output-state semanticSeededOutput20-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics20-P1-onlyG.txt --output-topic-keys semanticSeededKeys20-P1-onlyG.txt 
```

30 topics
```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 30 --output-state semanticSeededOutput30-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics30-P1-onlyG.txt --output-topic-keys semanticSeededKeys30-P1-onlyG.txt --word-topic-counts-file semanticSeededCounts30-P1-onlyG.txt --topic-word-weights-file semanticSeededWeights30-P1.txt --diagnostics-file semanticSeededDiagnostics30-P1-onlyG.txt --num-top-words 500
```

```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 30 --output-state semanticSeededOutput30-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics30-P1-onlyG.txt --output-topic-keys semanticSeededKeys30-P1-onlyG.txt --word-topic-counts-file semanticSeededCounts30-P1-onlyG.txt --topic-word-weights-file semanticSeededWeights30-P1.txt --diagnostics-file semanticSeededDiagnostics30-P1-onlyG.txt --num-top-words 500
```

```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --input-state semanticSeededOutput30-P1-onlyG.gz --diagnostics-file semanticSeededDiagnostics30-P1-onlyG.txt --
```


40 topics
```
bin\mallet train-topics --input semantic-seeded-P1-onlyG.mallet --num-topics 40 --output-state semanticSeededOutput40-P1-onlyG.gz --optimize-interval 10 --optimize-burn-in 20 --output-doc-topics semanticSeededTopics40-P1-onlyG.txt --output-topic-keys semanticSeededKeys40-P1-onlyG.txt  
```



#### Analysis of results
```
Oh this is lovely!
2:57
Look at the Center panel.
2:57
You can see how the diversity now has the Marina curve.
2:57
Low, then rising, then falling
2:58
You can also see how the diversity peaks earlier for the most coarse-grained case.
```
```
By the way — you can use the coarse-graining axioms of Information Theory!
3:10
For example, if you take the difference between the n=20 and n=10 lines, you are measuring “the amount of fine-grained diversity”.
3:11
This will tell us about how the community is “focusing down” (possibly in different subdomains)
3:13
This would allow you to track a really interesting semantic distinction
```

#### Polymath data README

- Column names
–