# Feature Visualization

![rw-book-cover](https://distill.pub/2017/feature-visualization/thumbnail.jpg)

# Metadata
- Author: [[Chris Olah]]
- Full Title: Feature Visualization

- URL: https://distill.pub/2017/feature-visualization/

# Highlights
- A given feature of a network may respond to a wide range of inputs. On the class level, for example, a classifier that has been trained to recognize dogs should recognize both closeups of their faces as well as wider profile images — even though those have quite different visual appearances. ([View Highlight](https://read.readwise.io/read/01h6dz895j6psqvebbcqzqdsr6))
- A different approach by Nguyen, Yosinski, and collaborators was to search through the dataset for diverse examples and use those as starting points for the optimization process . The idea is that this initiates optimization in different facets of the feature so that the resulting example from optimization will demonstrate that facet. In more recent work, they combine visualizing classes with a generative model, which they can sample for diverse examples . ([View Highlight](https://read.readwise.io/read/01h6dzdyecf52nr0mx7qwfzggd))
- We find there’s a very simple way to achieve diversity: adding a “diversity term” to one’s objective that pushes multiple examples to be different from each other. The diversity term can take a variety of forms, and we don’t have much understanding of their benefits yet. ([View Highlight](https://read.readwise.io/read/01h6dzgj75s2k5kt1xhfa23r3h))
- Diverse feature visualizations allow us to more closely pinpoint what activates a neuron, to the degree that we can make, and — by looking at dataset examples — *check* predictions about what inputs will activate the neuron. ([View Highlight](https://read.readwise.io/read/01h6dzmjcrz6x0awq81q9f2rr7))
    - Note: Here, diverse describing the objective, and feature visualization as a result? Still sort of confused about what all the terms mean... The examples in following paragraphs are very helpful!
- This simpler approach has a number of shortcomings: For one, the pressure to make examples different can cause unrelated artifacts (such as eyes) to appear. Additionally, the optimization may make examples be different in an unnatural way. ([View Highlight](https://read.readwise.io/read/01h6dzs9sf5c9cca20k5gzc0h2))
    - Note: Ultimately, if the goal is visualizing the NN's representation, how can we say it is unnatural? Ideal is to be learning the "real world" as best as possible, but perhaps this is just the "natural" extreme of network function?
- A helpful way to think about these combinations is geometrically: let’s define *activation space* to be all possible combinations of neuron activations. ([View Highlight](https://read.readwise.io/read/01h6dzzp60zdesfpc9rwzndztm))
- We can then think of individual neuron activations as the *basis vectors* of this activation space. Conversely, a combination of neuron activations is then just a vector in this space. ([View Highlight](https://read.readwise.io/read/01h6dzzy18wq6tdvffrfav5xrf))
- We can also define interesting directions in activation space by doing arithmetic on neurons. For example, if we add a “black and white” neuron to a “mosaic” neuron, we obtain a black and white version of the mosaic. This is reminiscent of semantic arithmetic of word embeddings as seen in Word2Vec or generative models’ latent spaces. ([View Highlight](https://read.readwise.io/read/01h6e01kppc39rnkdrvse92cbv))
    - Note: Whoa! Why does this happen? Latent space = "a compressed, abstract representation of the dataset."
- These examples show us how neurons jointly represent images. To better understand how neurons interact, we can also interpolate between them. [The optimization objective is a linear interpolation between the individual channel objectives. To get the interpolations to look better, we also add a small alignment objective that encourages lower layer activations to be similar. We additionally use a combination of separate and shared image parameterizations to make it easier for the optimization algorithm to cause objects to line up, while still giving it the freedom to create any image it needs to.] This is similar to interpolating in the latent space of generative models. ([View Highlight](https://read.readwise.io/read/01h6e0bt045d61mjp8ryjg3jr1))
- We don’t fully understand why these high frequency patterns form, but an important part seems to be strided convolutions and pooling operations, which create high-frequency patterns in the gradient . ([View Highlight](https://read.readwise.io/read/01h6e0phexjn23fdnxmpp7jftt))
    - Note: Somehow relevant to the pooling/no pool difference in tuning/gradient correlation seen? (summer 2023 research project)
- **Weak Regularization** avoids misleading correlations, but is less connected to real use. ([View Highlight](https://read.readwise.io/read/01h6e0kbp6g28pztzy7b14tw4q))
- **Strong Regularization** gives more realistic examples at risk of misleading correlations ([View Highlight](https://read.readwise.io/read/01h6e0kk5b5xyd1w5mczzwryxb))
- **Transformation robustness** tries to find examples that still activate the optimization target highly even if we slightly transform them. Even a small amount seems to be very effective in the case of images , especially when combined with a more general regularizer for high-frequencies . ([View Highlight](https://read.readwise.io/read/01h6e0vz8x4f6h38jdk2b6earr))
    - Note: This method makes the most "sense" to me.
- Our previous regularizers use very simple heuristics to keep examples reasonable. A natural next step is to actually learn a model of the real data and try to enforce that. With a strong model, this becomes similar to searching over the dataset. This approach produces the most photorealistic visualizations, but it may be unclear what came from the model being visualized and what came from the prior. ([View Highlight](https://read.readwise.io/read/01h6e19y213ds93fad50a4gbb2))
- Transforming the gradient like this is actually quite a powerful tool — it’s called “preconditioning” in optimization. You can think of it as doing steepest descent to optimize the same objective, but in another parameterization of the space or under a different notion of distance. ([View Highlight](https://read.readwise.io/read/01h6e1brzbvft26njp5ad4rz9n))
    - Note: Much more intuitive description than the linear algebra understanding I had previously -- cool!
- There remains still a lot of important work to be done in improving feature visualization. Some issues that stand out include understanding neuron interaction, finding which units are most meaningful for understanding neural net activations, and giving a holistic view of the facets of a feature. ([View Highlight](https://read.readwise.io/read/01h6e1h1k7fpag9jwvd60d43sw))
